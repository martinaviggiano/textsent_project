{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. DATA IMPORT\n",
    "Hate speech dataset from a white supremacist forum:  https://github.com/Vicomtech/hate-speech-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import ntpath\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import nltk.sentiment.vader as vd\n",
    "import string\n",
    "import re\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import download\n",
    "from langdetect import detect\n",
    "import spacy\n",
    "from wordcloud import WordCloud\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, Ridge\n",
    "from sklearn.feature_selection import SelectKBest, chi2, RFE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    plot_confusion_matrix,\n",
    "    plot_precision_recall_curve,\n",
    "    plot_roc_curve,\n",
    ")\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "import sklearn as sk\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as VS\n",
    "import imblearn\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from collections import Counter\n",
    "import preprocessor as pproc\n",
    "from cleantext import clean\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marti\\anaconda3\\lib\\site-packages\\tqdm\\std.py:697: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\marti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files_paths = glob.glob(\"hate-speech-dataset/all_files/*.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files_paths = [f for f in all_files_paths if os.path.isfile(f)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files_names = [str(ntpath.basename(f)).replace(\".txt\", \"\") for f in all_files_paths]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Error removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d30366318524dbabef9bafa41bceae6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=10944.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "txt_content = {}\n",
    "errors = []\n",
    "for name, path in tqdm(list(zip(all_files_names, all_files_paths))):\n",
    "    with open(path, \"r\") as txt:\n",
    "        try:\n",
    "            txt_content[name] = txt.readline().replace(\",\", \"\")\n",
    "        except Exception as ex:\n",
    "            errors.append((name, str(ex)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_list = [err[0] for err in errors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(txt_content, orient='index').reset_index()\n",
    "df.columns = [\"file_id\",\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann = pd.read_csv('hate-speech-dataset/annotations_metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann = ann[~ann['file_id'].isin(errors_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(left=ann, right=df, left_on='file_id', right_on='file_id')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.loc[(data[\"label\"] != \"relation\") & (data[\"label\"] != \"idk/skip\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns=[\"file_id\", \"user_id\", \"subforum_id\", \"num_contexts\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['label'] = data.apply(lambda x: 0 if x['label'] == \"noHate\" else 1, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. CLEANING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Removing links, tags, numbers and bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_text_clean(text):\n",
    "    cList = {\n",
    "        \"n't\": \" not\",\n",
    "        \"/TD\": \" \",\n",
    "        \" PM \": \" personal message \",\n",
    "        \" pm \": \" personal message \",\n",
    "        \"PM \": \"personal message \",\n",
    "        \" Donot \": \" do not \",\n",
    "        \" MB \": \" megabytes \",\n",
    "    }\n",
    "\n",
    "    c_re = re.compile(\"(%s)\" % \"|\".join(cList.keys()))\n",
    "    \n",
    "    def expand_contractions(text, c_re=c_re):\n",
    "        def replace(match):\n",
    "            return cList[match.group(0)]\n",
    "\n",
    "        return c_re.sub(replace, text)\n",
    "    \n",
    "    qualcosa = expand_contractions(text)\n",
    "    \n",
    "    qualcosaltro = pproc.clean(\n",
    "            clean(\n",
    "                pproc.clean(qualcosa),\n",
    "                fix_unicode=True,  # fix various unicode errors\n",
    "                to_ascii=True,  # transliterate to closest ASCII representation\n",
    "                lower=True,  # lowercase text\n",
    "                no_line_breaks=True,  # fully strip line breaks as opposed to only normalizing them\n",
    "                no_urls=True,  # replace all URLs with a special token\n",
    "                no_emails=True,  # replace all email addresses with a special token\n",
    "                no_phone_numbers=True,  # replace all phone numbers with a special token\n",
    "                no_numbers=True,  # replace all numbers with a special token\n",
    "                no_digits=True,  # replace all digits with a special token\n",
    "                no_currency_symbols=True,  # replace all currency symbols with a special token\n",
    "                no_punct=True,\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    swords = set().union(stopwords.words(\"english\"), string.punctuation)\n",
    "\n",
    "    altroancora = (\n",
    "        qualcosaltro\n",
    "        .lower()\n",
    "        .replace(r\"(@[a-z0-9]+)\\w+\", \" \")\n",
    "        .replace(r\"http\\S+\", \"\")\n",
    "        .replace(r\"www\\S+\", \" \")\n",
    "        .replace(r\"com/watch\", \" \")\n",
    "        .replace(r\"\\S*[.,:;!?-]\\S*[^\\s\\.,:;!?-]\", \" \")\n",
    "        .replace(r\" th \", \" \")\n",
    "        .replace(r\" th \", \" \")\n",
    "        .replace(r\"\\w*\\d\\w*\", \" \")\n",
    "        .replace(r\"rlm\", \" \")\n",
    "        .replace(r\"pttm\", \" \")\n",
    "        .replace(r\"ghlight\", \" \")\n",
    "        .replace(r\"[0-9]+(?:st| st|nd| nd|rd| rd|th| th)\", \"\")\n",
    "        .replace(r\"([^a-z \\t])\", \" \")\n",
    "        .replace(r\" +\", \" \")\n",
    "        )\n",
    "    \n",
    "    altroancora = \" \".join([i for i in altroancora.split() if not i in swords])\n",
    "    \n",
    "    return altroancora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text_clean'] = [full_text_clean(text) for text in data['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['word_count_before'] = data['text'].apply(lambda x: len(x.split())) # Number of words in the string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['word_count'] = data['text_clean'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['word_cleaning'] = data['word_count_before'] - data['word_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['word_count_before','word_count','word_cleaning']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.loc[data['word_count'] > 0, ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Lemmatizer and Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = nlp.pipe(data['text_clean'], n_process=2, batch_size=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['spacy_doc'] = [x for x in tqdm(docs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['POS_spacy'] = data['spacy_doc'].progress_apply(lambda x: [(y.text, y.pos_) for y in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['lemmatized'] = data['spacy_doc'].progress_apply(lambda x: \" \".join([y.lemma_ for y in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tokens'] = data['spacy_doc'].progress_apply(lambda x: [y.text for y in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['language'] = data['spacy_doc'].progress_apply(lambda x: set([y.lang_ for y in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = {}\n",
    "for line in data['language']:\n",
    "    if len(line) in length:\n",
    "        length[len(line)] += 1\n",
    "    else:\n",
    "        length[len(line)] = 1\n",
    "length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['language'] = data['language'].progress_apply(lambda x: list(x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['language'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_text_pos(x):\n",
    "    final_pos_text = []\n",
    "    for elem in x:\n",
    "        for pos in pos_list:\n",
    "            if elem[1] == pos:\n",
    "                final_pos_text.append(elem[0])\n",
    "    \n",
    "    return \" \".join(final_pos_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_list = [\"NOUN\"]\n",
    "data[\"NOUN\"] = data.apply(lambda x: filter_text_pos(x[\"POS_spacy\"]), axis=1)\n",
    "data['NOUN_count'] = data['NOUN'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_list = [\"PROPN\"]\n",
    "data[\"PROPN\"] = data.apply(lambda x: filter_text_pos(x[\"POS_spacy\"]), axis=1)\n",
    "data['PROPN_count'] = data['PROPN'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_list = [\"VERB\"]\n",
    "data[\"VERB\"] = data.apply(lambda x: filter_text_pos(x[\"POS_spacy\"]), axis=1)\n",
    "data['VERB_count'] = data['VERB'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_list = [\"ADJ\"]\n",
    "data[\"ADJ\"] = data.apply(lambda x: filter_text_pos(x[\"POS_spacy\"]), axis=1)\n",
    "data['ADJ_count'] = data['ADJ'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_list = [\"ADV\"]\n",
    "data[\"ADV\"] = data.apply(lambda x: filter_text_pos(x[\"POS_spacy\"]), axis=1)\n",
    "data['ADV_count'] = data['ADV'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_list = [\"PRON\"]\n",
    "data[\"PRON\"] = data.apply(lambda x: filter_text_pos(x[\"POS_spacy\"]), axis=1)\n",
    "data['PRON_count'] = data['PRON'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_list = [\"SCONJ\"]\n",
    "data[\"SCONJ\"] = data.apply(lambda x: filter_text_pos(x[\"POS_spacy\"]), axis=1)\n",
    "data['SCONJ_count'] = data['SCONJ'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_list = [\"INTJ\"]\n",
    "data[\"INTJ\"] = data.apply(lambda x: filter_text_pos(x[\"POS_spacy\"]), axis=1)\n",
    "data['INTJ_count'] = data['SCONJ'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
