{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hate speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files_paths = glob.glob(\"hate-speech-dataset/all_files/*.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files_paths = [f for f in all_files_paths if os.path.isfile(f)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files_names = [f.replace(\"hate-speech-dataset/all_files\\\\\", \"\").replace(\".txt\", \"\") for f in all_files_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10944/10944 [00:04<00:00, 2518.80it/s]\n"
     ]
    }
   ],
   "source": [
    "txt_content = {}\n",
    "errors = []\n",
    "for name, path in tqdm(list(zip(all_files_names, all_files_paths))):\n",
    "    with open(path, \"r\") as txt:\n",
    "        try:\n",
    "            txt_content[name] = txt.readline()\n",
    "        except Exception as ex:\n",
    "            errors.append((name, str(ex)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('13491591_4',\n",
       "  \"'charmap' codec can't decode byte 0x81 in position 23: character maps to <undefined>\"),\n",
       " ('13493456_1',\n",
       "  \"'charmap' codec can't decode byte 0x81 in position 169: character maps to <undefined>\"),\n",
       " ('13500400_1',\n",
       "  \"'charmap' codec can't decode byte 0x81 in position 67: character maps to <undefined>\"),\n",
       " ('13560796_1',\n",
       "  \"'charmap' codec can't decode byte 0x81 in position 5: character maps to <undefined>\"),\n",
       " ('13572081_1',\n",
       "  \"'charmap' codec can't decode byte 0x81 in position 3: character maps to <undefined>\"),\n",
       " ('13588794_3',\n",
       "  \"'charmap' codec can't decode byte 0x81 in position 54: character maps to <undefined>\"),\n",
       " ('13595072_1',\n",
       "  \"'charmap' codec can't decode byte 0x8d in position 75: character maps to <undefined>\"),\n",
       " ('13595072_5',\n",
       "  \"'charmap' codec can't decode byte 0x8d in position 78: character maps to <undefined>\"),\n",
       " ('14061724_1',\n",
       "  \"'charmap' codec can't decode byte 0x81 in position 181: character maps to <undefined>\"),\n",
       " ('14065492_1',\n",
       "  \"'charmap' codec can't decode byte 0x90 in position 203: character maps to <undefined>\"),\n",
       " ('30422455_1',\n",
       "  \"'charmap' codec can't decode byte 0x8d in position 94: character maps to <undefined>\"),\n",
       " ('30422455_2',\n",
       "  \"'charmap' codec can't decode byte 0x8d in position 65: character maps to <undefined>\"),\n",
       " ('30443860_2',\n",
       "  \"'charmap' codec can't decode byte 0x90 in position 20: character maps to <undefined>\"),\n",
       " ('30569404_3',\n",
       "  \"'charmap' codec can't decode byte 0x8d in position 55: character maps to <undefined>\"),\n",
       " ('30586603_2',\n",
       "  \"'charmap' codec can't decode byte 0x8d in position 235: character maps to <undefined>\"),\n",
       " ('30659540_3',\n",
       "  \"'charmap' codec can't decode byte 0x8f in position 68: character maps to <undefined>\"),\n",
       " ('33500574_1',\n",
       "  \"'charmap' codec can't decode byte 0x90 in position 1: character maps to <undefined>\"),\n",
       " ('33500574_2',\n",
       "  \"'charmap' codec can't decode byte 0x90 in position 1: character maps to <undefined>\")]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_list = [err[0] for err in errors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['13491591_4',\n",
       " '13493456_1',\n",
       " '13500400_1',\n",
       " '13560796_1',\n",
       " '13572081_1',\n",
       " '13588794_3',\n",
       " '13595072_1',\n",
       " '13595072_5',\n",
       " '14061724_1',\n",
       " '14065492_1',\n",
       " '30422455_1',\n",
       " '30422455_2',\n",
       " '30443860_2',\n",
       " '30569404_3',\n",
       " '30586603_2',\n",
       " '30659540_3',\n",
       " '33500574_1',\n",
       " '33500574_2']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(txt_content, orient='index').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = [\"file_id\", \"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12834217_1</td>\n",
       "      <td>As of March 13th , 2014 , the booklet had been...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12834217_10</td>\n",
       "      <td>Thank you in advance. : ) Download the youtube...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12834217_2</td>\n",
       "      <td>In order to help increase the booklets downloa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12834217_3</td>\n",
       "      <td>( Simply copy and paste the following text int...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12834217_4</td>\n",
       "      <td>Click below for a FREE download of a colorfull...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       file_id                                               text\n",
       "0   12834217_1  As of March 13th , 2014 , the booklet had been...\n",
       "1  12834217_10  Thank you in advance. : ) Download the youtube...\n",
       "2   12834217_2  In order to help increase the booklets downloa...\n",
       "3   12834217_3  ( Simply copy and paste the following text int...\n",
       "4   12834217_4  Click below for a FREE download of a colorfull..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann = pd.read_csv('hate-speech-dataset/annotations_metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>subforum_id</th>\n",
       "      <th>num_contexts</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12834217_1</td>\n",
       "      <td>572066</td>\n",
       "      <td>1346</td>\n",
       "      <td>0</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12834217_2</td>\n",
       "      <td>572066</td>\n",
       "      <td>1346</td>\n",
       "      <td>0</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12834217_3</td>\n",
       "      <td>572066</td>\n",
       "      <td>1346</td>\n",
       "      <td>0</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12834217_4</td>\n",
       "      <td>572066</td>\n",
       "      <td>1346</td>\n",
       "      <td>0</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12834217_5</td>\n",
       "      <td>572066</td>\n",
       "      <td>1346</td>\n",
       "      <td>0</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10939</th>\n",
       "      <td>33676864_5</td>\n",
       "      <td>734541</td>\n",
       "      <td>1388</td>\n",
       "      <td>0</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10940</th>\n",
       "      <td>33677019_1</td>\n",
       "      <td>735154</td>\n",
       "      <td>1388</td>\n",
       "      <td>0</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10941</th>\n",
       "      <td>33677019_2</td>\n",
       "      <td>735154</td>\n",
       "      <td>1388</td>\n",
       "      <td>0</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10942</th>\n",
       "      <td>33677053_1</td>\n",
       "      <td>572266</td>\n",
       "      <td>1388</td>\n",
       "      <td>0</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10943</th>\n",
       "      <td>33677053_2</td>\n",
       "      <td>572266</td>\n",
       "      <td>1388</td>\n",
       "      <td>0</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10944 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          file_id  user_id  subforum_id  num_contexts   label\n",
       "0      12834217_1   572066         1346             0  noHate\n",
       "1      12834217_2   572066         1346             0  noHate\n",
       "2      12834217_3   572066         1346             0  noHate\n",
       "3      12834217_4   572066         1346             0    hate\n",
       "4      12834217_5   572066         1346             0  noHate\n",
       "...           ...      ...          ...           ...     ...\n",
       "10939  33676864_5   734541         1388             0  noHate\n",
       "10940  33677019_1   735154         1388             0  noHate\n",
       "10941  33677019_2   735154         1388             0  noHate\n",
       "10942  33677053_1   572266         1388             0    hate\n",
       "10943  33677053_2   572266         1388             0  noHate\n",
       "\n",
       "[10944 rows x 5 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann = ann[~ann['file_id'].isin(errors_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>subforum_id</th>\n",
       "      <th>num_contexts</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12834217_1</td>\n",
       "      <td>572066</td>\n",
       "      <td>1346</td>\n",
       "      <td>0</td>\n",
       "      <td>noHate</td>\n",
       "      <td>As of March 13th , 2014 , the booklet had been...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12834217_2</td>\n",
       "      <td>572066</td>\n",
       "      <td>1346</td>\n",
       "      <td>0</td>\n",
       "      <td>noHate</td>\n",
       "      <td>In order to help increase the booklets downloa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12834217_3</td>\n",
       "      <td>572066</td>\n",
       "      <td>1346</td>\n",
       "      <td>0</td>\n",
       "      <td>noHate</td>\n",
       "      <td>( Simply copy and paste the following text int...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12834217_4</td>\n",
       "      <td>572066</td>\n",
       "      <td>1346</td>\n",
       "      <td>0</td>\n",
       "      <td>hate</td>\n",
       "      <td>Click below for a FREE download of a colorfull...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12834217_5</td>\n",
       "      <td>572066</td>\n",
       "      <td>1346</td>\n",
       "      <td>0</td>\n",
       "      <td>noHate</td>\n",
       "      <td>Click on the `` DOWNLOAD ( 7.42 MB ) '' green ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10921</th>\n",
       "      <td>33676864_5</td>\n",
       "      <td>734541</td>\n",
       "      <td>1388</td>\n",
       "      <td>0</td>\n",
       "      <td>noHate</td>\n",
       "      <td>Billy - `` That guy would n't leave me alone ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10922</th>\n",
       "      <td>33677019_1</td>\n",
       "      <td>735154</td>\n",
       "      <td>1388</td>\n",
       "      <td>0</td>\n",
       "      <td>noHate</td>\n",
       "      <td>Wish we at least had a Marine Le Pen to vote f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10923</th>\n",
       "      <td>33677019_2</td>\n",
       "      <td>735154</td>\n",
       "      <td>1388</td>\n",
       "      <td>0</td>\n",
       "      <td>noHate</td>\n",
       "      <td>Its like the choices are white genocide candid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10924</th>\n",
       "      <td>33677053_1</td>\n",
       "      <td>572266</td>\n",
       "      <td>1388</td>\n",
       "      <td>0</td>\n",
       "      <td>hate</td>\n",
       "      <td>Why White people used to say that sex was a si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10925</th>\n",
       "      <td>33677053_2</td>\n",
       "      <td>572266</td>\n",
       "      <td>1388</td>\n",
       "      <td>0</td>\n",
       "      <td>noHate</td>\n",
       "      <td>Now I get it !</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10926 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          file_id  user_id  subforum_id  num_contexts   label  \\\n",
       "0      12834217_1   572066         1346             0  noHate   \n",
       "1      12834217_2   572066         1346             0  noHate   \n",
       "2      12834217_3   572066         1346             0  noHate   \n",
       "3      12834217_4   572066         1346             0    hate   \n",
       "4      12834217_5   572066         1346             0  noHate   \n",
       "...           ...      ...          ...           ...     ...   \n",
       "10921  33676864_5   734541         1388             0  noHate   \n",
       "10922  33677019_1   735154         1388             0  noHate   \n",
       "10923  33677019_2   735154         1388             0  noHate   \n",
       "10924  33677053_1   572266         1388             0    hate   \n",
       "10925  33677053_2   572266         1388             0  noHate   \n",
       "\n",
       "                                                    text  \n",
       "0      As of March 13th , 2014 , the booklet had been...  \n",
       "1      In order to help increase the booklets downloa...  \n",
       "2      ( Simply copy and paste the following text int...  \n",
       "3      Click below for a FREE download of a colorfull...  \n",
       "4      Click on the `` DOWNLOAD ( 7.42 MB ) '' green ...  \n",
       "...                                                  ...  \n",
       "10921  Billy - `` That guy would n't leave me alone ,...  \n",
       "10922  Wish we at least had a Marine Le Pen to vote f...  \n",
       "10923  Its like the choices are white genocide candid...  \n",
       "10924  Why White people used to say that sex was a si...  \n",
       "10925                                     Now I get it !  \n",
       "\n",
       "[10926 rows x 6 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.merge(left=ann, right=df, left_on='file_id', right_on='file_id')\n",
    "data.shape\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['label'] = data.apply(lambda x: 0 if x['label'] == \"noHate\" else 1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>subforum_id</th>\n",
       "      <th>num_contexts</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12834217_1</td>\n",
       "      <td>572066</td>\n",
       "      <td>1346</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>As of March 13th , 2014 , the booklet had been...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12834217_2</td>\n",
       "      <td>572066</td>\n",
       "      <td>1346</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>In order to help increase the booklets downloa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12834217_3</td>\n",
       "      <td>572066</td>\n",
       "      <td>1346</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>( Simply copy and paste the following text int...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12834217_4</td>\n",
       "      <td>572066</td>\n",
       "      <td>1346</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Click below for a FREE download of a colorfull...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12834217_5</td>\n",
       "      <td>572066</td>\n",
       "      <td>1346</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Click on the `` DOWNLOAD ( 7.42 MB ) '' green ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10921</th>\n",
       "      <td>33676864_5</td>\n",
       "      <td>734541</td>\n",
       "      <td>1388</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Billy - `` That guy would n't leave me alone ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10922</th>\n",
       "      <td>33677019_1</td>\n",
       "      <td>735154</td>\n",
       "      <td>1388</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Wish we at least had a Marine Le Pen to vote f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10923</th>\n",
       "      <td>33677019_2</td>\n",
       "      <td>735154</td>\n",
       "      <td>1388</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Its like the choices are white genocide candid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10924</th>\n",
       "      <td>33677053_1</td>\n",
       "      <td>572266</td>\n",
       "      <td>1388</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Why White people used to say that sex was a si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10925</th>\n",
       "      <td>33677053_2</td>\n",
       "      <td>572266</td>\n",
       "      <td>1388</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Now I get it !</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10926 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          file_id  user_id  subforum_id  num_contexts  label  \\\n",
       "0      12834217_1   572066         1346             0      0   \n",
       "1      12834217_2   572066         1346             0      0   \n",
       "2      12834217_3   572066         1346             0      0   \n",
       "3      12834217_4   572066         1346             0      1   \n",
       "4      12834217_5   572066         1346             0      0   \n",
       "...           ...      ...          ...           ...    ...   \n",
       "10921  33676864_5   734541         1388             0      0   \n",
       "10922  33677019_1   735154         1388             0      0   \n",
       "10923  33677019_2   735154         1388             0      0   \n",
       "10924  33677053_1   572266         1388             0      1   \n",
       "10925  33677053_2   572266         1388             0      0   \n",
       "\n",
       "                                                    text  \n",
       "0      As of March 13th , 2014 , the booklet had been...  \n",
       "1      In order to help increase the booklets downloa...  \n",
       "2      ( Simply copy and paste the following text int...  \n",
       "3      Click below for a FREE download of a colorfull...  \n",
       "4      Click on the `` DOWNLOAD ( 7.42 MB ) '' green ...  \n",
       "...                                                  ...  \n",
       "10921  Billy - `` That guy would n't leave me alone ,...  \n",
       "10922  Wish we at least had a Marine Le Pen to vote f...  \n",
       "10923  Its like the choices are white genocide candid...  \n",
       "10924  Why White people used to say that sex was a si...  \n",
       "10925                                     Now I get it !  \n",
       "\n",
       "[10926 rows x 6 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\marti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "import nltk.sentiment.vader as vd\n",
    "from nltk import download\n",
    "download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional  = ['rt','rts','retweet']\n",
    "swords = set().union(stopwords.words('english'), additional, string.punctuation)\n",
    "data['text_clean'] = data['text'].str.lower()\\\n",
    "    .str.replace('(@[a-z0-9]+)\\w+',' ')\\\n",
    "    .str.replace('(http\\S+)', ' ')\\\n",
    "    .str.replace('[0-9]+(?:st| st|nd| nd|rd| rd|th| th)', '')\\\n",
    "    .str.replace('([^a-z \\t])',' ')\\\n",
    "    .str.replace(' +',' ')\\\n",
    "    .apply(lambda x: \" \".join([i for i in x.split() if not i in swords]))\n",
    "#any alpha numeric after @ #http # any value except alpha numeric tab character ('\\u0009')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming\n",
    "ps = PorterStemmer()\n",
    "data['stemmed'] = data['text_clean'].apply(lambda x: \" \".join([ps.stem(i) for i in word_tokenize(x)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatizer\n",
    "lm = nltk.WordNetLemmatizer()\n",
    "data['lemmatized'] = data['text_clean'].apply(lambda x: \" \".join([lm.lemmatize(i) for i in word_tokenize(x)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Analysis by nltk VADER\n",
    "sia = vd.SentimentIntensityAnalyzer()\n",
    "from nltk.tokenize import word_tokenize\n",
    "data['sentiment_score'] = data['stemmed'].apply(lambda x: sum([sia.polarity_scores(i)['compound'] for i in word_tokenize(x)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>subforum_id</th>\n",
       "      <th>num_contexts</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>sentiment_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12834217_1</td>\n",
       "      <td>572066</td>\n",
       "      <td>1346</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>As of March 13th , 2014 , the booklet had been...</td>\n",
       "      <td>march booklet downloaded times counting</td>\n",
       "      <td>march booklet download time count</td>\n",
       "      <td>march booklet downloaded time counting</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12834217_2</td>\n",
       "      <td>572066</td>\n",
       "      <td>1346</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>In order to help increase the booklets downloa...</td>\n",
       "      <td>order help increase booklets downloads would g...</td>\n",
       "      <td>order help increas booklet download would grea...</td>\n",
       "      <td>order help increase booklet downloads would gr...</td>\n",
       "      <td>1.0268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12834217_3</td>\n",
       "      <td>572066</td>\n",
       "      <td>1346</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>( Simply copy and paste the following text int...</td>\n",
       "      <td>simply copy paste following text youtube video...</td>\n",
       "      <td>simpli copi past follow text youtub video desc...</td>\n",
       "      <td>simply copy paste following text youtube video...</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12834217_4</td>\n",
       "      <td>572066</td>\n",
       "      <td>1346</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Click below for a FREE download of a colorfull...</td>\n",
       "      <td>click free download colorfully illustrated pag...</td>\n",
       "      <td>click free download color illustr page e book ...</td>\n",
       "      <td>click free download colorfully illustrated pag...</td>\n",
       "      <td>-0.0161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12834217_5</td>\n",
       "      <td>572066</td>\n",
       "      <td>1346</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Click on the `` DOWNLOAD ( 7.42 MB ) '' green ...</td>\n",
       "      <td>click download mb green banner link</td>\n",
       "      <td>click download mb green banner link</td>\n",
       "      <td>click download mb green banner link</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10921</th>\n",
       "      <td>33676864_5</td>\n",
       "      <td>734541</td>\n",
       "      <td>1388</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Billy - `` That guy would n't leave me alone ,...</td>\n",
       "      <td>billy guy would n leave alone gave trudeau salute</td>\n",
       "      <td>billi guy would n leav alon gave trudeau salut</td>\n",
       "      <td>billy guy would n leave alone gave trudeau salute</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10922</th>\n",
       "      <td>33677019_1</td>\n",
       "      <td>735154</td>\n",
       "      <td>1388</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Wish we at least had a Marine Le Pen to vote f...</td>\n",
       "      <td>wish least marine le pen vote canada</td>\n",
       "      <td>wish least marin le pen vote canada</td>\n",
       "      <td>wish least marine le pen vote canada</td>\n",
       "      <td>0.4019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10923</th>\n",
       "      <td>33677019_2</td>\n",
       "      <td>735154</td>\n",
       "      <td>1388</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Its like the choices are white genocide candid...</td>\n",
       "      <td>like choices white genocide candidate</td>\n",
       "      <td>like choic white genocid candid</td>\n",
       "      <td>like choice white genocide candidate</td>\n",
       "      <td>0.3612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10924</th>\n",
       "      <td>33677053_1</td>\n",
       "      <td>572266</td>\n",
       "      <td>1388</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Why White people used to say that sex was a si...</td>\n",
       "      <td>white people used say sex sin used mystery saw...</td>\n",
       "      <td>white peopl use say sex sin use mysteri saw ch...</td>\n",
       "      <td>white people used say sex sin used mystery saw...</td>\n",
       "      <td>-0.5574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10925</th>\n",
       "      <td>33677053_2</td>\n",
       "      <td>572266</td>\n",
       "      <td>1388</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Now I get it !</td>\n",
       "      <td>get</td>\n",
       "      <td>get</td>\n",
       "      <td>get</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10926 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          file_id  user_id  subforum_id  num_contexts  label  \\\n",
       "0      12834217_1   572066         1346             0      0   \n",
       "1      12834217_2   572066         1346             0      0   \n",
       "2      12834217_3   572066         1346             0      0   \n",
       "3      12834217_4   572066         1346             0      1   \n",
       "4      12834217_5   572066         1346             0      0   \n",
       "...           ...      ...          ...           ...    ...   \n",
       "10921  33676864_5   734541         1388             0      0   \n",
       "10922  33677019_1   735154         1388             0      0   \n",
       "10923  33677019_2   735154         1388             0      0   \n",
       "10924  33677053_1   572266         1388             0      1   \n",
       "10925  33677053_2   572266         1388             0      0   \n",
       "\n",
       "                                                    text  \\\n",
       "0      As of March 13th , 2014 , the booklet had been...   \n",
       "1      In order to help increase the booklets downloa...   \n",
       "2      ( Simply copy and paste the following text int...   \n",
       "3      Click below for a FREE download of a colorfull...   \n",
       "4      Click on the `` DOWNLOAD ( 7.42 MB ) '' green ...   \n",
       "...                                                  ...   \n",
       "10921  Billy - `` That guy would n't leave me alone ,...   \n",
       "10922  Wish we at least had a Marine Le Pen to vote f...   \n",
       "10923  Its like the choices are white genocide candid...   \n",
       "10924  Why White people used to say that sex was a si...   \n",
       "10925                                     Now I get it !   \n",
       "\n",
       "                                              text_clean  \\\n",
       "0                march booklet downloaded times counting   \n",
       "1      order help increase booklets downloads would g...   \n",
       "2      simply copy paste following text youtube video...   \n",
       "3      click free download colorfully illustrated pag...   \n",
       "4                    click download mb green banner link   \n",
       "...                                                  ...   \n",
       "10921  billy guy would n leave alone gave trudeau salute   \n",
       "10922               wish least marine le pen vote canada   \n",
       "10923              like choices white genocide candidate   \n",
       "10924  white people used say sex sin used mystery saw...   \n",
       "10925                                                get   \n",
       "\n",
       "                                                 stemmed  \\\n",
       "0                      march booklet download time count   \n",
       "1      order help increas booklet download would grea...   \n",
       "2      simpli copi past follow text youtub video desc...   \n",
       "3      click free download color illustr page e book ...   \n",
       "4                    click download mb green banner link   \n",
       "...                                                  ...   \n",
       "10921     billi guy would n leav alon gave trudeau salut   \n",
       "10922                wish least marin le pen vote canada   \n",
       "10923                    like choic white genocid candid   \n",
       "10924  white peopl use say sex sin use mysteri saw ch...   \n",
       "10925                                                get   \n",
       "\n",
       "                                              lemmatized  sentiment_score  \n",
       "0                 march booklet downloaded time counting           0.0000  \n",
       "1      order help increase booklet downloads would gr...           1.0268  \n",
       "2      simply copy paste following text youtube video...           0.0000  \n",
       "3      click free download colorfully illustrated pag...          -0.0161  \n",
       "4                    click download mb green banner link           0.0000  \n",
       "...                                                  ...              ...  \n",
       "10921  billy guy would n leave alone gave trudeau salute           0.0000  \n",
       "10922               wish least marine le pen vote canada           0.4019  \n",
       "10923               like choice white genocide candidate           0.3612  \n",
       "10924  white people used say sex sin used mystery saw...          -0.5574  \n",
       "10925                                                get           0.0000  \n",
       "\n",
       "[10926 rows x 10 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.    ,  1.0268, -0.0161, ..., -0.6918,  0.7524,  0.3384])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sentiment_score.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.    ,  1.0268, -0.0161, ..., -0.6918,  0.7524,  0.3384])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sentiment_score.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.10434749298690595\n"
     ]
    }
   ],
   "source": [
    "col1 = data['label']\n",
    "col2 = data['sentiment_score']\n",
    "correlation_df = col1.corr(col2)\n",
    "print(correlation_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RELATION BTW LABEL AND SENTIMETN_SCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bow vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "bow_vectorizer = CountVectorizer(max_features=100)\n",
    "X_bow = bow_vectorizer.fit_transform(data['lemmatized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ago', 'also', 'always', 'anyone', 'anything', 'area', 'around', 'back', 'black', 'child', 'com', 'come', 'could', 'country', 'day', 'dont', 'even', 'ever', 'every', 'eye', 'find', 'first', 'forum', 'friend', 'get', 'girl', 'go', 'going', 'good', 'got', 'great', 'group', 'guy', 'home', 'hope', 'im', 'ireland', 'jew', 'kid', 'know', 'last', 'let', 'like', 'little', 'live', 'long', 'look', 'looking', 'lot', 'make', 'man', 'many', 'maybe', 'much', 'nationalist', 'need', 'negro', 'never', 'new', 'news', 'non', 'old', 'one', 'people', 'place', 'post', 'put', 'race', 'read', 'really', 'right', 'said', 'say', 'school', 'see', 'show', 'someone', 'something', 'start', 'stormfront', 'take', 'thing', 'think', 'thread', 'time', 'two', 'video', 'want', 'watch', 'way', 'week', 'well', 'white', 'woman', 'work', 'world', 'would', 'www', 'year', 'youtube']\n"
     ]
    }
   ],
   "source": [
    "print(bow_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ago', 'also', 'always', 'anyone', 'anything', 'area', 'around', 'back', 'black', 'child', 'com', 'come', 'could', 'country', 'day', 'dont', 'even', 'ever', 'every', 'eye', 'find', 'first', 'forum', 'friend', 'get', 'girl', 'go', 'going', 'good', 'got', 'great', 'group', 'guy', 'home', 'hope', 'im', 'ireland', 'jew', 'kid', 'know', 'last', 'let', 'like', 'little', 'live', 'long', 'look', 'looking', 'lot', 'make', 'man', 'many', 'maybe', 'much', 'nationalist', 'need', 'negro', 'never', 'new', 'news', 'non', 'old', 'one', 'people', 'place', 'post', 'put', 'race', 'read', 'really', 'right', 'said', 'say', 'school', 'see', 'show', 'someone', 'something', 'start', 'stormfront', 'take', 'thing', 'think', 'thread', 'time', 'two', 'video', 'want', 'watch', 'way', 'week', 'well', 'white', 'woman', 'work', 'world', 'would', 'www', 'year', 'youtube']\n"
     ]
    }
   ],
   "source": [
    "# Tfidf vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=100)\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(data['lemmatized'])\n",
    "print(tfidf_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>white</th>\n",
       "      <th>back</th>\n",
       "      <th>www</th>\n",
       "      <th>com</th>\n",
       "      <th>world</th>\n",
       "      <th>jew</th>\n",
       "      <th>think</th>\n",
       "      <th>black</th>\n",
       "      <th>every</th>\n",
       "      <th>get</th>\n",
       "      <th>look</th>\n",
       "      <th>hope</th>\n",
       "      <th>one</th>\n",
       "      <th>say</th>\n",
       "      <th>read</th>\n",
       "      <th>well</th>\n",
       "      <th>always</th>\n",
       "      <th>like</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.383608</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.280128</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32937</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.377809</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.472976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.382452</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      white      back  www      com  world       jew  think  black  every  \\\n",
       "0  0.000000  0.000000  0.0  0.00000    0.0  0.383608    0.0    0.0    0.0   \n",
       "1  0.000000  0.000000  0.0  0.00000    0.0  0.000000    0.0    0.0    0.0   \n",
       "2  0.000000  0.000000  0.0  0.00000    0.0  0.000000    0.0    0.0    0.0   \n",
       "3  0.000000  0.000000  0.0  0.00000    0.0  0.000000    0.0    0.0    0.0   \n",
       "4  0.000000  0.000000  0.0  0.00000    0.0  0.000000    0.0    0.0    0.0   \n",
       "5  0.000000  0.000000  0.0  0.00000    0.0  0.000000    0.0    0.0    0.0   \n",
       "6  0.280128  0.000000  0.0  0.32937    0.0  0.000000    0.0    0.0    0.0   \n",
       "7  0.000000  0.000000  0.0  0.00000    0.0  0.000000    0.0    0.0    0.0   \n",
       "8  0.000000  0.000000  0.0  0.00000    0.0  0.000000    0.0    0.0    0.0   \n",
       "9  0.000000  0.377809  0.0  0.00000    0.0  0.000000    0.0    0.0    0.0   \n",
       "\n",
       "   get  look      hope  one  say  read  well  always      like  \n",
       "0  0.0   0.0  0.000000  0.0  0.0   0.0   0.0     0.0  0.000000  \n",
       "1  0.0   0.0  0.000000  0.0  0.0   0.0   0.0     0.0  0.000000  \n",
       "2  0.0   0.0  0.000000  0.0  0.0   0.0   0.0     0.0  0.000000  \n",
       "3  0.0   0.0  0.000000  0.0  0.0   0.0   0.0     0.0  0.000000  \n",
       "4  0.0   0.0  0.000000  0.0  0.0   0.0   0.0     0.0  0.000000  \n",
       "5  0.0   0.0  0.000000  0.0  0.0   0.0   0.0     0.0  0.000000  \n",
       "6  0.0   0.0  0.000000  0.0  0.0   0.0   0.0     0.0  0.000000  \n",
       "7  0.0   0.0  0.000000  0.0  0.0   0.0   0.0     0.0  0.000000  \n",
       "8  0.0   0.0  0.000000  0.0  0.0   0.0   0.0     0.0  0.000000  \n",
       "9  0.0   0.0  0.472976  0.0  0.0   0.0   0.0     0.0  0.382452  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X_tfidf[1:11,7:25].toarray(), columns=list(tfidf_vectorizer.vocabulary_.keys())[7:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/@cristhianboujon/how-to-list-the-most-common-words-from-text-corpus-using-scikit-learn-dad4d0cab41d\n",
    "def get_top_n_words(corpus, n=None):\n",
    "    vec = CountVectorizer().fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "white 1596\n",
      "like 803\n",
      "people 630\n",
      "black 619\n",
      "one 569\n",
      "get 523\n",
      "would 515\n",
      "youtube 484\n",
      "year 420\n",
      "time 396\n",
      "see 377\n",
      "good 374\n",
      "school 359\n",
      "go 355\n",
      "know 350\n",
      "say 311\n",
      "day 301\n",
      "think 299\n",
      "want 292\n",
      "kid 272\n",
      "new 272\n",
      "race 270\n",
      "back 265\n",
      "look 260\n",
      "well 244\n",
      "make 239\n",
      "way 231\n",
      "video 230\n",
      "jew 228\n",
      "many 226\n",
      "thing 224\n",
      "old 224\n",
      "country 223\n",
      "got 212\n",
      "need 211\n",
      "around 208\n",
      "even 205\n",
      "never 203\n",
      "show 196\n",
      "child 193\n",
      "could 191\n",
      "every 191\n",
      "going 188\n",
      "great 183\n",
      "find 182\n",
      "post 181\n",
      "also 178\n",
      "come 177\n",
      "let 176\n",
      "woman 175\n"
     ]
    }
   ],
   "source": [
    "common_words = get_top_n_words(data['lemmatized'], 50)\n",
    "for word, freq in common_words:\n",
    "    print(word, freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tokens'] = data['text_clean'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS\n",
    "from textblob import TextBlob\n",
    "def pos_tag(text):\n",
    "    try:\n",
    "        return TextBlob(text).tags\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['POS'] = data['lemmatized'].apply(pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adjectives(text):\n",
    "    blob = TextBlob(text)\n",
    "    return [word for (word,tag) in blob.tags if tag == \"JJ\" or tag == \"JJR\" or tag == \"JJS\"]\n",
    "\n",
    "data['ADJ'] = data['lemmatized'].apply(get_adjectives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                       []\n",
       "1                                            [great, text]\n",
       "2                                                   [copy]\n",
       "3        [click, free, illustrated, e, intentional, wes...\n",
       "4                                                  [green]\n",
       "                               ...                        \n",
       "10921                                            [trudeau]\n",
       "10922                           [wish, least, marine, pen]\n",
       "10923                                              [white]\n",
       "10924                         [white, child, brown, mixed]\n",
       "10925                                                   []\n",
       "Name: lemmatized, Length: 10926, dtype: object"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj = data['lemmatized'].apply(get_adjectives)\n",
    "adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['great',\n",
       " 'text',\n",
       " 'copy',\n",
       " 'click',\n",
       " 'free',\n",
       " 'illustrated',\n",
       " 'e',\n",
       " 'intentional',\n",
       " 'western',\n",
       " 'green',\n",
       " 'western',\n",
       " 'white',\n",
       " 'yt',\n",
       " 'txt',\n",
       " 'interested',\n",
       " 'spread',\n",
       " 'copy',\n",
       " 'info',\n",
       " 'stupid',\n",
       " 'black',\n",
       " 'alongside',\n",
       " 'white',\n",
       " 'dirty',\n",
       " 'graphic',\n",
       " 'official',\n",
       " 'white',\n",
       " 'higher',\n",
       " 'white',\n",
       " 'free',\n",
       " 'latest',\n",
       " 'write',\n",
       " 'neighbourhood',\n",
       " 'sad',\n",
       " 'white',\n",
       " 'right',\n",
       " 'young',\n",
       " 'angry',\n",
       " 'favorite',\n",
       " 'old',\n",
       " 'many',\n",
       " 'national',\n",
       " 'geographic',\n",
       " 'early',\n",
       " 'replete',\n",
       " 'african',\n",
       " 'albert',\n",
       " 'documentary',\n",
       " 'available',\n",
       " 'pale',\n",
       " 'suppose',\n",
       " 'n',\n",
       " 'cultural',\n",
       " 'girl',\n",
       " 'guy',\n",
       " 'middle',\n",
       " 'lip',\n",
       " 'bad',\n",
       " 'whole',\n",
       " 'bizarre',\n",
       " 'supporter',\n",
       " 'sound',\n",
       " 'israeli',\n",
       " 'know',\n",
       " 'many',\n",
       " 'israel',\n",
       " 'give',\n",
       " 'u',\n",
       " 'fox',\n",
       " 'israel',\n",
       " 'possible',\n",
       " 'free',\n",
       " 'particular',\n",
       " 'perfect',\n",
       " 'start',\n",
       " 'national',\n",
       " 'political',\n",
       " 'free',\n",
       " 'first',\n",
       " 'n',\n",
       " 'long',\n",
       " 'pathetic',\n",
       " 'little',\n",
       " 'innocent',\n",
       " 'white',\n",
       " 'white',\n",
       " 'gm',\n",
       " 'entire',\n",
       " 'ok',\n",
       " 'free',\n",
       " 'informative',\n",
       " 'intentional',\n",
       " 'white',\n",
       " 'wntube',\n",
       " 'net',\n",
       " 'contribute',\n",
       " 'texas',\n",
       " 'much',\n",
       " 'welcome',\n",
       " 'beautiful',\n",
       " 'white',\n",
       " 'scum',\n",
       " 'scum',\n",
       " 'military',\n",
       " 'stupid',\n",
       " 'nice',\n",
       " 'suppose',\n",
       " 'cow',\n",
       " 'female',\n",
       " 'cow',\n",
       " 'much',\n",
       " 'mad',\n",
       " 'shed',\n",
       " 'tear',\n",
       " 'black',\n",
       " 'unsubscribed',\n",
       " 'white',\n",
       " 'little',\n",
       " 'black',\n",
       " 'nbc',\n",
       " 'sad',\n",
       " 'sorry',\n",
       " 'several',\n",
       " 'hard',\n",
       " 'black',\n",
       " 'white',\n",
       " 'ne',\n",
       " 'asian',\n",
       " 'indian',\n",
       " 'mestizo',\n",
       " 'hundred',\n",
       " 'black',\n",
       " 'major',\n",
       " 'black',\n",
       " 'black',\n",
       " 'black',\n",
       " 'hear',\n",
       " 'kid',\n",
       " 'public',\n",
       " 'poor',\n",
       " 'white',\n",
       " 'white',\n",
       " 'saturday',\n",
       " 'foot',\n",
       " 'excellent',\n",
       " 'old',\n",
       " 'free',\n",
       " 'reporter',\n",
       " 'old',\n",
       " 'free',\n",
       " 'harnett',\n",
       " 'fourth',\n",
       " 'suspect',\n",
       " 'prior',\n",
       " 'latest',\n",
       " 'burglary',\n",
       " 'larceny',\n",
       " 'suspect',\n",
       " 'new',\n",
       " 'groid',\n",
       " 'obama',\n",
       " 'black',\n",
       " 'next',\n",
       " 'die',\n",
       " 'good',\n",
       " 'white',\n",
       " 'single',\n",
       " 'single',\n",
       " 'retarded',\n",
       " 'feral',\n",
       " 'feral',\n",
       " 'upright',\n",
       " 'human',\n",
       " 'negro',\n",
       " 'filthy',\n",
       " 'armenian',\n",
       " 'large',\n",
       " 'white',\n",
       " 'white',\n",
       " 'german',\n",
       " 'jewish',\n",
       " 'front',\n",
       " 'white',\n",
       " 'public',\n",
       " 'american',\n",
       " 'ok',\n",
       " 'thread',\n",
       " 'animal',\n",
       " 'human',\n",
       " 'baby',\n",
       " 'n',\n",
       " 'equal',\n",
       " 'non',\n",
       " 'white',\n",
       " 'white',\n",
       " 'white',\n",
       " 'much',\n",
       " 'white',\n",
       " 'act',\n",
       " 'black',\n",
       " 'sexual',\n",
       " 'black',\n",
       " 'white',\n",
       " 'white',\n",
       " 'little',\n",
       " 'little',\n",
       " 'pride',\n",
       " 'stick',\n",
       " 'white',\n",
       " 'different',\n",
       " 'white',\n",
       " 'white',\n",
       " 'fight',\n",
       " 'white',\n",
       " 'white',\n",
       " 'christian',\n",
       " 'give',\n",
       " 'precipitous',\n",
       " 'ive',\n",
       " 'black',\n",
       " 'london',\n",
       " 'original',\n",
       " 'black',\n",
       " 'south',\n",
       " 'london',\n",
       " 'south',\n",
       " 'african',\n",
       " 'overseas',\n",
       " 'sa',\n",
       " 'british',\n",
       " 'different',\n",
       " 'stuff',\n",
       " 'different',\n",
       " 'mar',\n",
       " 'diet',\n",
       " 'aspartame',\n",
       " 'avoid',\n",
       " 'n',\n",
       " 'hard',\n",
       " 'high',\n",
       " 'fructose',\n",
       " 'bad',\n",
       " 'soya',\n",
       " 'fresh',\n",
       " 'edible',\n",
       " 'safe',\n",
       " 'lucky',\n",
       " 'chemical',\n",
       " 'actual',\n",
       " 'black',\n",
       " 'young',\n",
       " 'white',\n",
       " 'une',\n",
       " 'fille',\n",
       " 'vid',\n",
       " 'chicken',\n",
       " 'rainforest',\n",
       " 'video',\n",
       " 'full',\n",
       " 'white',\n",
       " 'muslim',\n",
       " 'michigan',\n",
       " 'high',\n",
       " 'hijab',\n",
       " 'able',\n",
       " 'finish',\n",
       " 'hear',\n",
       " 'uttering',\n",
       " 'gibberish',\n",
       " 'free',\n",
       " 'hard',\n",
       " 'average',\n",
       " 'white',\n",
       " 'disenfranchisement',\n",
       " 'planned',\n",
       " 'much',\n",
       " 'white',\n",
       " 'cultural',\n",
       " 'white',\n",
       " 'ethnic',\n",
       " 'white',\n",
       " 'american',\n",
       " 'ethnic',\n",
       " 'long',\n",
       " 'ive',\n",
       " 'comercial',\n",
       " 'u',\n",
       " 'anti',\n",
       " 'european',\n",
       " 'black',\n",
       " 'laptop',\n",
       " 'white',\n",
       " 'next',\n",
       " 'laptop',\n",
       " 'laptop',\n",
       " 'couch',\n",
       " 'implying',\n",
       " 'sad',\n",
       " 'many',\n",
       " 'white',\n",
       " 'happy',\n",
       " 'brady',\n",
       " 'good',\n",
       " 'white',\n",
       " 'nordic',\n",
       " 'white',\n",
       " 'celebrate',\n",
       " 'black',\n",
       " 'black',\n",
       " 'pathological',\n",
       " 'biblical',\n",
       " 'bad',\n",
       " 'tribe',\n",
       " 'white',\n",
       " 'beautiful',\n",
       " 'white',\n",
       " 'sick',\n",
       " 'white',\n",
       " 'white',\n",
       " 'american',\n",
       " 'white',\n",
       " 'amish',\n",
       " 'white',\n",
       " 'large',\n",
       " 'better',\n",
       " 'private',\n",
       " 'average',\n",
       " 'parent',\n",
       " 'poor',\n",
       " 'ultra',\n",
       " 'strict',\n",
       " 'private',\n",
       " 'white',\n",
       " 'regular',\n",
       " 'local',\n",
       " 'sunny',\n",
       " 'forum',\n",
       " 'loud',\n",
       " 'first',\n",
       " 'white',\n",
       " 'concerned',\n",
       " 'u',\n",
       " 'n',\n",
       " 'white',\n",
       " 'european',\n",
       " 'american',\n",
       " 'black',\n",
       " 'animal',\n",
       " 'update',\n",
       " 'david',\n",
       " 'den',\n",
       " 'genocide',\n",
       " 'ivanovich',\n",
       " 'january',\n",
       " 'forest',\n",
       " 'swiss',\n",
       " 'forest',\n",
       " 'forest',\n",
       " 'grey',\n",
       " 'mixed',\n",
       " 'blond',\n",
       " 'blue',\n",
       " 'ethnical',\n",
       " 'finn',\n",
       " 'dark',\n",
       " 'dark',\n",
       " 'short',\n",
       " 'high',\n",
       " 'lowest',\n",
       " 'cheap',\n",
       " 'tchaikovsky',\n",
       " 'russian',\n",
       " 'traditional',\n",
       " 'russian',\n",
       " 'orchestra',\n",
       " 'orchestra',\n",
       " 'ensemble',\n",
       " 'sad',\n",
       " 'long',\n",
       " 'many',\n",
       " 'little',\n",
       " 'many',\n",
       " 'ukraine',\n",
       " 'famous',\n",
       " 'traditional',\n",
       " 'live',\n",
       " 'memory',\n",
       " 'subject',\n",
       " 'untill',\n",
       " 'next',\n",
       " 'lodz',\n",
       " 'untill',\n",
       " 'start',\n",
       " 'great',\n",
       " 'fit',\n",
       " 'open',\n",
       " 'unnecessary',\n",
       " 'hispanic',\n",
       " 'old',\n",
       " 'several',\n",
       " 'negro',\n",
       " 'old',\n",
       " 'brian',\n",
       " 'brownie',\n",
       " 'porn',\n",
       " 'black',\n",
       " 'hair',\n",
       " 'black',\n",
       " 'criminal',\n",
       " 'olde',\n",
       " 'cornielious',\n",
       " 'odin',\n",
       " 'odin',\n",
       " 'give',\n",
       " 'much',\n",
       " 'want',\n",
       " 'n',\n",
       " 'russian',\n",
       " 'christian',\n",
       " 'different',\n",
       " 'january',\n",
       " 'gentile',\n",
       " 'couple',\n",
       " 'negroid',\n",
       " 'couple',\n",
       " 'moblie',\n",
       " 'irika',\n",
       " 'last',\n",
       " 'update',\n",
       " 'moblie',\n",
       " 'local',\n",
       " 'fondling',\n",
       " 'alert',\n",
       " 'monday',\n",
       " 'est',\n",
       " 'website',\n",
       " 'interesting',\n",
       " 'statistic',\n",
       " 'black',\n",
       " 'ubiquity',\n",
       " 'black',\n",
       " 'professional',\n",
       " 'ebay',\n",
       " 'sure',\n",
       " 'genuine',\n",
       " 'steal',\n",
       " 'good',\n",
       " 'swedish',\n",
       " 'white',\n",
       " 'average',\n",
       " 'white',\n",
       " 'swedish',\n",
       " 'swede',\n",
       " 'best',\n",
       " 'great',\n",
       " 'norwegian',\n",
       " 'u',\n",
       " 'gingerbread',\n",
       " 'various',\n",
       " 'present',\n",
       " 'tree',\n",
       " 'open',\n",
       " 'interesting',\n",
       " 'last',\n",
       " 'cool',\n",
       " 'lol',\n",
       " 'specific',\n",
       " 'sture',\n",
       " 'n',\n",
       " 'svenska',\n",
       " 'wrong',\n",
       " 'angry',\n",
       " 'black',\n",
       " 'sweden',\n",
       " 'live',\n",
       " 'pagan',\n",
       " 'til',\n",
       " 'rkneyjar',\n",
       " 'sure',\n",
       " 'ten',\n",
       " 'stay',\n",
       " 'sweden',\n",
       " 'much',\n",
       " 'important',\n",
       " 'u',\n",
       " 'u',\n",
       " 'legal',\n",
       " 'canada',\n",
       " 'non',\n",
       " 'white',\n",
       " 'good',\n",
       " 'sweden',\n",
       " 'low',\n",
       " 'higher',\n",
       " 'high',\n",
       " 'worst',\n",
       " 'norwegian',\n",
       " 'lower',\n",
       " 'good',\n",
       " 'black',\n",
       " 'mix',\n",
       " 'black',\n",
       " 'possible',\n",
       " 'non',\n",
       " 'white',\n",
       " 'grandma',\n",
       " 'early',\n",
       " 'new',\n",
       " 'much',\n",
       " 'n',\n",
       " 'f',\n",
       " 'afraid',\n",
       " 'drunk',\n",
       " 'sweden',\n",
       " 'welcome',\n",
       " 'ugric',\n",
       " 'non',\n",
       " 'irish',\n",
       " 'shoulder',\n",
       " 'long',\n",
       " 'russian',\n",
       " 'real',\n",
       " 'non',\n",
       " 'irish',\n",
       " 'wrong',\n",
       " 'immigrant',\n",
       " 'wrong',\n",
       " 'old',\n",
       " 'king',\n",
       " 'patrick',\n",
       " 'wish',\n",
       " 'irish',\n",
       " 'many',\n",
       " 'stupid',\n",
       " 'full',\n",
       " 'soldier',\n",
       " 'pole',\n",
       " 'irish',\n",
       " 'u',\n",
       " 'potential',\n",
       " 'celtic',\n",
       " 'pride',\n",
       " 'contact',\n",
       " 'different',\n",
       " 'willing',\n",
       " 'good',\n",
       " 'good',\n",
       " 'dont',\n",
       " 'thread',\n",
       " 'english',\n",
       " 'irish',\n",
       " 'neutral',\n",
       " 'nazi',\n",
       " 'common',\n",
       " 'thread',\n",
       " 'late',\n",
       " 'mix',\n",
       " 'intense',\n",
       " 'nigerian',\n",
       " 'assorted',\n",
       " 'sectarian',\n",
       " 'british',\n",
       " 'irish',\n",
       " 'admire',\n",
       " 'irish',\n",
       " 'total',\n",
       " 'early',\n",
       " 'sudden',\n",
       " 'least',\n",
       " 'arse',\n",
       " 'warrior',\n",
       " 'fellow',\n",
       " 'white',\n",
       " 'white',\n",
       " 'list',\n",
       " 'info',\n",
       " 'interesting',\n",
       " 'younger',\n",
       " 'gang',\n",
       " 'regular',\n",
       " 'hell',\n",
       " 'lose',\n",
       " 'east',\n",
       " 'germany',\n",
       " 'bother',\n",
       " 'btw',\n",
       " 'amnesty',\n",
       " 'much',\n",
       " 'lot',\n",
       " 'high',\n",
       " 'good',\n",
       " 'c',\n",
       " 'last',\n",
       " 'beautiful',\n",
       " 'good',\n",
       " 'true',\n",
       " 'black',\n",
       " 'white',\n",
       " 'nigerian',\n",
       " 'true',\n",
       " 'foreign',\n",
       " 'past',\n",
       " 'irish',\n",
       " 'rough',\n",
       " 'old',\n",
       " 'let',\n",
       " 'able',\n",
       " 'build',\n",
       " 'irish',\n",
       " 'republican',\n",
       " 'lie',\n",
       " 'irish',\n",
       " 'mainstream',\n",
       " 'active',\n",
       " 'net',\n",
       " 'touch',\n",
       " 'multi',\n",
       " 'cultural',\n",
       " 'wrong',\n",
       " 'hated',\n",
       " 'yeah',\n",
       " 'calgary',\n",
       " 'open',\n",
       " 'native',\n",
       " 'unusual',\n",
       " 'white',\n",
       " 'new',\n",
       " 'true',\n",
       " 'biracial',\n",
       " 'due',\n",
       " 'similar',\n",
       " 'eg',\n",
       " 'fifa',\n",
       " 'british',\n",
       " 'somalia',\n",
       " 'camel',\n",
       " 'english',\n",
       " 'worse',\n",
       " 'english',\n",
       " 'good',\n",
       " 'small',\n",
       " 'lot',\n",
       " 'various',\n",
       " 'willing',\n",
       " 'highlight',\n",
       " 'el',\n",
       " 'english',\n",
       " 'locust',\n",
       " 'live',\n",
       " 'desire',\n",
       " 'english',\n",
       " 'small',\n",
       " 'christian',\n",
       " 'meek',\n",
       " 'earth',\n",
       " 'peaceful',\n",
       " 'total',\n",
       " 'happy',\n",
       " 'eternal',\n",
       " 'english',\n",
       " 'embrace',\n",
       " 'simple',\n",
       " 'preparing',\n",
       " 'future',\n",
       " 'ridiculous',\n",
       " 'advocate',\n",
       " 'sovereign',\n",
       " 'sunnier',\n",
       " 'clime',\n",
       " 'explain',\n",
       " 'much',\n",
       " 'new',\n",
       " 'serbia',\n",
       " 'u',\n",
       " 'secondary',\n",
       " 'full',\n",
       " 'black',\n",
       " 'english',\n",
       " 'disgust',\n",
       " 'regular',\n",
       " 'laugh',\n",
       " 'negro',\n",
       " 'want',\n",
       " 'give',\n",
       " 'africa',\n",
       " 'dose',\n",
       " 'bnp',\n",
       " 'next',\n",
       " 'warm',\n",
       " 'guess',\n",
       " 'immigrant',\n",
       " 'african',\n",
       " 'many',\n",
       " 'front',\n",
       " 'flat',\n",
       " 'small',\n",
       " 'ball',\n",
       " 'right',\n",
       " 'hear',\n",
       " 'bad',\n",
       " 'dog',\n",
       " 'stupid',\n",
       " 'sat',\n",
       " 'black',\n",
       " 'white',\n",
       " 'new',\n",
       " 'foreign',\n",
       " 'open',\n",
       " 'stench',\n",
       " 'vent',\n",
       " 'angry',\n",
       " 'white',\n",
       " 'white',\n",
       " 'absolute',\n",
       " 'black',\n",
       " 'homosexual',\n",
       " 'silent',\n",
       " 'speak',\n",
       " 'free',\n",
       " 'ridiculous',\n",
       " 'new',\n",
       " 'active',\n",
       " 'online',\n",
       " 'follow',\n",
       " 'toilet',\n",
       " 'new',\n",
       " 'agree',\n",
       " 'many',\n",
       " 'english',\n",
       " 'white',\n",
       " 'many',\n",
       " 'white',\n",
       " 'ultras',\n",
       " 'cant',\n",
       " 'understand',\n",
       " 'non',\n",
       " 'white',\n",
       " 'mysterious',\n",
       " 'die',\n",
       " 'ok',\n",
       " 'white',\n",
       " 'u',\n",
       " 'lemon',\n",
       " 'alien',\n",
       " 'black',\n",
       " 'white',\n",
       " 'light',\n",
       " 'andre',\n",
       " 'french',\n",
       " 'critic',\n",
       " 'black',\n",
       " 'nordic',\n",
       " 'true',\n",
       " 'incident',\n",
       " 'muslim',\n",
       " 'anglian',\n",
       " 'afghanistan',\n",
       " 'ta',\n",
       " 'white',\n",
       " 'white',\n",
       " 'least',\n",
       " 'hard',\n",
       " 'personal',\n",
       " 'upper',\n",
       " 'silesian',\n",
       " 'dramastein',\n",
       " 'third',\n",
       " 'worst',\n",
       " 'second',\n",
       " 'u',\n",
       " 'posada',\n",
       " 'budapest',\n",
       " 'military',\n",
       " 'superior',\n",
       " 'ancestral',\n",
       " 'great',\n",
       " 'politician',\n",
       " 'utterly',\n",
       " 'muslim',\n",
       " 'dead',\n",
       " 'friend',\n",
       " 'white',\n",
       " 'foreign',\n",
       " 'chinese',\n",
       " 'dive',\n",
       " 'enough',\n",
       " 'transnistria',\n",
       " 'hard',\n",
       " 'economical',\n",
       " 'accomplish',\n",
       " 'gay',\n",
       " 'non',\n",
       " 'local',\n",
       " 'pride',\n",
       " 'many',\n",
       " 'non',\n",
       " 'romanian',\n",
       " 'western',\n",
       " 'vast',\n",
       " 'romanian',\n",
       " 'n',\n",
       " 'great',\n",
       " 'polish',\n",
       " 'sad',\n",
       " 'hear',\n",
       " 'white',\n",
       " 'ill',\n",
       " 'bulgarian',\n",
       " 'seek',\n",
       " 'new',\n",
       " 'ukraine',\n",
       " 'ukrainian',\n",
       " 'csabi',\n",
       " 'romanian',\n",
       " 'hungarian',\n",
       " 'molecular',\n",
       " 'genetic',\n",
       " 'small',\n",
       " 'hungarian',\n",
       " 'indian',\n",
       " 'central',\n",
       " 'european',\n",
       " 'human',\n",
       " 'chromosomal',\n",
       " 'hungarian',\n",
       " 'orsolya',\n",
       " 'forensic',\n",
       " 'budapest',\n",
       " 'hungarian',\n",
       " 'forensic',\n",
       " 'budapest',\n",
       " 'hungary',\n",
       " 'hungarian',\n",
       " 'exact',\n",
       " 'sensationalist',\n",
       " 'stormfront',\n",
       " 'hungarian',\n",
       " 'likely',\n",
       " 'n',\n",
       " 'memorial',\n",
       " 'red',\n",
       " 'top',\n",
       " 'ten',\n",
       " 'irish',\n",
       " 'ireland',\n",
       " 'top',\n",
       " 'irishcentral',\n",
       " 'catholic',\n",
       " 'little',\n",
       " 'last',\n",
       " 'n',\n",
       " 'liberal',\n",
       " 'great',\n",
       " 'ant',\n",
       " 'n',\n",
       " 'logic',\n",
       " 'willing',\n",
       " 'give',\n",
       " 'much',\n",
       " 'stromfront',\n",
       " 'white',\n",
       " 'afreka',\n",
       " 'slav',\n",
       " 'live',\n",
       " 'western',\n",
       " 'devil',\n",
       " 'american',\n",
       " 'stupid',\n",
       " 'little',\n",
       " 'gathering',\n",
       " 'good',\n",
       " 'old',\n",
       " 'candle',\n",
       " 'good',\n",
       " 'forbidden',\n",
       " 'im',\n",
       " 'new',\n",
       " 'good',\n",
       " 'send',\n",
       " 'great',\n",
       " 'lucky',\n",
       " 'awesome',\n",
       " 'fun',\n",
       " 'white',\n",
       " 'lady',\n",
       " 'fine',\n",
       " 'bad',\n",
       " 'onstage',\n",
       " 'u',\n",
       " 'good',\n",
       " 'haired',\n",
       " 'third',\n",
       " 'married',\n",
       " 'last',\n",
       " 'best',\n",
       " 'right',\n",
       " 'pillow',\n",
       " 'little',\n",
       " 'lethal',\n",
       " 'hi',\n",
       " 'cal',\n",
       " 'many',\n",
       " 'minded',\n",
       " 'click',\n",
       " 'old',\n",
       " 'new',\n",
       " 'ive',\n",
       " 'last',\n",
       " 'dont',\n",
       " 'cant',\n",
       " 'im',\n",
       " 'dont',\n",
       " 'profile',\n",
       " 'eurodate',\n",
       " 'pic',\n",
       " 'small',\n",
       " 'new',\n",
       " 'hole',\n",
       " 'sure',\n",
       " 'real',\n",
       " 'fake',\n",
       " 'pretty',\n",
       " 'dead',\n",
       " 'good',\n",
       " 'hey',\n",
       " 'excited',\n",
       " 'full',\n",
       " 'minded',\n",
       " 'socal',\n",
       " 'u',\n",
       " 'proud',\n",
       " 'nice',\n",
       " 'opp',\n",
       " 'awhile',\n",
       " 'reserve',\n",
       " 'bust',\n",
       " 'busy',\n",
       " 'live',\n",
       " 'tiny',\n",
       " 'white',\n",
       " 'le',\n",
       " 'san',\n",
       " 'antonio',\n",
       " 'many',\n",
       " 'stick',\n",
       " 'white',\n",
       " 'girl',\n",
       " 'ta',\n",
       " 'lack',\n",
       " 'white',\n",
       " 'proud',\n",
       " 'white',\n",
       " 'im',\n",
       " 'live',\n",
       " 'last',\n",
       " 'yr',\n",
       " 'cute',\n",
       " 'white',\n",
       " 'e',\n",
       " 'old',\n",
       " 'female',\n",
       " 'live',\n",
       " 'poor',\n",
       " 'fair',\n",
       " 'rich',\n",
       " 'good',\n",
       " 'interested',\n",
       " 'capable',\n",
       " 'brief',\n",
       " 'resume',\n",
       " 'att',\n",
       " 'net',\n",
       " 'acceptable',\n",
       " 'website',\n",
       " 'n',\n",
       " 'undesirable',\n",
       " 'prior',\n",
       " 'military',\n",
       " 'stormfront',\n",
       " 'ask',\n",
       " 'careful',\n",
       " 'interested',\n",
       " 'proud',\n",
       " 'white',\n",
       " 'green',\n",
       " 'white',\n",
       " 'good',\n",
       " 'good',\n",
       " 'first',\n",
       " 'n',\n",
       " 'standard',\n",
       " 'email',\n",
       " 'n',\n",
       " 'happy',\n",
       " 'puppy',\n",
       " 'white',\n",
       " 'thin',\n",
       " 'right',\n",
       " 'tall',\n",
       " 'proud',\n",
       " 'suppose',\n",
       " ...]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj_list = [j for i in adj for j in i]\n",
    "adj_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['white',\n",
       " 'black',\n",
       " 'good',\n",
       " 'new',\n",
       " 'u',\n",
       " 'n',\n",
       " 'many',\n",
       " 'old',\n",
       " 'great',\n",
       " 'last',\n",
       " 'live',\n",
       " 'much',\n",
       " 'little',\n",
       " 'american',\n",
       " 'free',\n",
       " 'asian',\n",
       " 'bad',\n",
       " 'irish',\n",
       " 'non',\n",
       " 'sure']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/3594514/how-to-find-most-common-elements-of-a-list\n",
    "adj_counter = {}\n",
    "for word in adj_list:\n",
    "    if word in adj_counter:\n",
    "        adj_counter[word] += 1\n",
    "    else:\n",
    "        adj_counter[word] = 1\n",
    "            \n",
    "popular_adj = sorted(adj_counter, key = adj_counter.get, reverse = True)\n",
    "top_20_adj = popular_adj[:20]\n",
    "top_20_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noun(text):\n",
    "    blob = TextBlob(text)\n",
    "    return [word for (word,tag) in blob.tags if tag == \"NN\" or tag == \"NNS\" or tag == \"NNP\" or tag == \"NNPS\"]\n",
    "\n",
    "data['NOUN'] = data['lemmatized'].apply(get_noun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['people',\n",
       " 'year',\n",
       " 'youtube',\n",
       " 'time',\n",
       " 'school',\n",
       " 'day',\n",
       " 'race',\n",
       " 'way',\n",
       " 'thing',\n",
       " 'country',\n",
       " 'kid',\n",
       " 'video',\n",
       " 'jew',\n",
       " 'n',\n",
       " 'woman',\n",
       " 'anyone',\n",
       " 'show',\n",
       " 'look',\n",
       " 'home',\n",
       " 'post']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noun = data['lemmatized'].apply(get_noun)\n",
    "noun_list = [j for i in noun for j in i]\n",
    "noun_counter = {}\n",
    "for word in noun_list:\n",
    "    if word in noun_counter:\n",
    "        noun_counter[word] += 1\n",
    "    else:\n",
    "        noun_counter[word] = 1\n",
    "            \n",
    "popular_noun = sorted(noun_counter, key = noun_counter.get, reverse = True)\n",
    "top_20_noun = popular_noun[:20]\n",
    "top_20_noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>subforum_id</th>\n",
       "      <th>num_contexts</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>tokens</th>\n",
       "      <th>POS</th>\n",
       "      <th>ADJ</th>\n",
       "      <th>NOUN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12834217_1</td>\n",
       "      <td>572066</td>\n",
       "      <td>1346</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>As of March 13th , 2014 , the booklet had been...</td>\n",
       "      <td>march booklet downloaded times counting</td>\n",
       "      <td>march booklet download time count</td>\n",
       "      <td>march booklet downloaded time counting</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>[march, booklet, downloaded, times, counting]</td>\n",
       "      <td>[(march, NN), (booklet, NN), (downloaded, VBD)...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[march, booklet, time, counting]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12834217_2</td>\n",
       "      <td>572066</td>\n",
       "      <td>1346</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>In order to help increase the booklets downloa...</td>\n",
       "      <td>order help increase booklets downloads would g...</td>\n",
       "      <td>order help increas booklet download would grea...</td>\n",
       "      <td>order help increase booklet downloads would gr...</td>\n",
       "      <td>1.0268</td>\n",
       "      <td>[order, help, increase, booklets, downloads, w...</td>\n",
       "      <td>[(order, NN), (help, NN), (increase, VB), (boo...</td>\n",
       "      <td>[great, text]</td>\n",
       "      <td>[order, help, booklet, downloads, stormfronter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12834217_3</td>\n",
       "      <td>572066</td>\n",
       "      <td>1346</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>( Simply copy and paste the following text int...</td>\n",
       "      <td>simply copy paste following text youtube video...</td>\n",
       "      <td>simpli copi past follow text youtub video desc...</td>\n",
       "      <td>simply copy paste following text youtube video...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>[simply, copy, paste, following, text, youtube...</td>\n",
       "      <td>[(simply, RB), (copy, JJ), (paste, NN), (follo...</td>\n",
       "      <td>[copy]</td>\n",
       "      <td>[paste, text, youtube, video, description, box]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12834217_4</td>\n",
       "      <td>572066</td>\n",
       "      <td>1346</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Click below for a FREE download of a colorfull...</td>\n",
       "      <td>click free download colorfully illustrated pag...</td>\n",
       "      <td>click free download color illustr page e book ...</td>\n",
       "      <td>click free download colorfully illustrated pag...</td>\n",
       "      <td>-0.0161</td>\n",
       "      <td>[click, free, download, colorfully, illustrate...</td>\n",
       "      <td>[(click, JJ), (free, JJ), (download, NN), (col...</td>\n",
       "      <td>[click, free, illustrated, e, intentional, wes...</td>\n",
       "      <td>[download, page, book, zionist, destruction, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12834217_5</td>\n",
       "      <td>572066</td>\n",
       "      <td>1346</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Click on the `` DOWNLOAD ( 7.42 MB ) '' green ...</td>\n",
       "      <td>click download mb green banner link</td>\n",
       "      <td>click download mb green banner link</td>\n",
       "      <td>click download mb green banner link</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>[click, download, mb, green, banner, link]</td>\n",
       "      <td>[(click, NN), (download, NN), (mb, NN), (green...</td>\n",
       "      <td>[green]</td>\n",
       "      <td>[click, download, mb, banner, link]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10921</th>\n",
       "      <td>33676864_5</td>\n",
       "      <td>734541</td>\n",
       "      <td>1388</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Billy - `` That guy would n't leave me alone ,...</td>\n",
       "      <td>billy guy would n leave alone gave trudeau salute</td>\n",
       "      <td>billi guy would n leav alon gave trudeau salut</td>\n",
       "      <td>billy guy would n leave alone gave trudeau salute</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>[billy, guy, would, n, leave, alone, gave, tru...</td>\n",
       "      <td>[(billy, RB), (guy, NN), (would, MD), (n, VB),...</td>\n",
       "      <td>[trudeau]</td>\n",
       "      <td>[guy, salute]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10922</th>\n",
       "      <td>33677019_1</td>\n",
       "      <td>735154</td>\n",
       "      <td>1388</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Wish we at least had a Marine Le Pen to vote f...</td>\n",
       "      <td>wish least marine le pen vote canada</td>\n",
       "      <td>wish least marin le pen vote canada</td>\n",
       "      <td>wish least marine le pen vote canada</td>\n",
       "      <td>0.4019</td>\n",
       "      <td>[wish, least, marine, le, pen, vote, canada]</td>\n",
       "      <td>[(wish, JJ), (least, JJS), (marine, JJ), (le, ...</td>\n",
       "      <td>[wish, least, marine, pen]</td>\n",
       "      <td>[le, vote, canada]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10923</th>\n",
       "      <td>33677019_2</td>\n",
       "      <td>735154</td>\n",
       "      <td>1388</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Its like the choices are white genocide candid...</td>\n",
       "      <td>like choices white genocide candidate</td>\n",
       "      <td>like choic white genocid candid</td>\n",
       "      <td>like choice white genocide candidate</td>\n",
       "      <td>0.3612</td>\n",
       "      <td>[like, choices, white, genocide, candidate]</td>\n",
       "      <td>[(like, IN), (choice, NN), (white, JJ), (genoc...</td>\n",
       "      <td>[white]</td>\n",
       "      <td>[choice, genocide, candidate]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10924</th>\n",
       "      <td>33677053_1</td>\n",
       "      <td>572266</td>\n",
       "      <td>1388</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Why White people used to say that sex was a si...</td>\n",
       "      <td>white people used say sex sin used mystery saw...</td>\n",
       "      <td>white peopl use say sex sin use mysteri saw ch...</td>\n",
       "      <td>white people used say sex sin used mystery saw...</td>\n",
       "      <td>-0.5574</td>\n",
       "      <td>[white, people, used, say, sex, sin, used, mys...</td>\n",
       "      <td>[(white, JJ), (people, NNS), (used, VBN), (say...</td>\n",
       "      <td>[white, child, brown, mixed]</td>\n",
       "      <td>[people, sex, sin, mystery, race, child]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10925</th>\n",
       "      <td>33677053_2</td>\n",
       "      <td>572266</td>\n",
       "      <td>1388</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Now I get it !</td>\n",
       "      <td>get</td>\n",
       "      <td>get</td>\n",
       "      <td>get</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>[get]</td>\n",
       "      <td>[(get, VB)]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10926 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          file_id  user_id  subforum_id  num_contexts  label  \\\n",
       "0      12834217_1   572066         1346             0      0   \n",
       "1      12834217_2   572066         1346             0      0   \n",
       "2      12834217_3   572066         1346             0      0   \n",
       "3      12834217_4   572066         1346             0      1   \n",
       "4      12834217_5   572066         1346             0      0   \n",
       "...           ...      ...          ...           ...    ...   \n",
       "10921  33676864_5   734541         1388             0      0   \n",
       "10922  33677019_1   735154         1388             0      0   \n",
       "10923  33677019_2   735154         1388             0      0   \n",
       "10924  33677053_1   572266         1388             0      1   \n",
       "10925  33677053_2   572266         1388             0      0   \n",
       "\n",
       "                                                    text  \\\n",
       "0      As of March 13th , 2014 , the booklet had been...   \n",
       "1      In order to help increase the booklets downloa...   \n",
       "2      ( Simply copy and paste the following text int...   \n",
       "3      Click below for a FREE download of a colorfull...   \n",
       "4      Click on the `` DOWNLOAD ( 7.42 MB ) '' green ...   \n",
       "...                                                  ...   \n",
       "10921  Billy - `` That guy would n't leave me alone ,...   \n",
       "10922  Wish we at least had a Marine Le Pen to vote f...   \n",
       "10923  Its like the choices are white genocide candid...   \n",
       "10924  Why White people used to say that sex was a si...   \n",
       "10925                                     Now I get it !   \n",
       "\n",
       "                                              text_clean  \\\n",
       "0                march booklet downloaded times counting   \n",
       "1      order help increase booklets downloads would g...   \n",
       "2      simply copy paste following text youtube video...   \n",
       "3      click free download colorfully illustrated pag...   \n",
       "4                    click download mb green banner link   \n",
       "...                                                  ...   \n",
       "10921  billy guy would n leave alone gave trudeau salute   \n",
       "10922               wish least marine le pen vote canada   \n",
       "10923              like choices white genocide candidate   \n",
       "10924  white people used say sex sin used mystery saw...   \n",
       "10925                                                get   \n",
       "\n",
       "                                                 stemmed  \\\n",
       "0                      march booklet download time count   \n",
       "1      order help increas booklet download would grea...   \n",
       "2      simpli copi past follow text youtub video desc...   \n",
       "3      click free download color illustr page e book ...   \n",
       "4                    click download mb green banner link   \n",
       "...                                                  ...   \n",
       "10921     billi guy would n leav alon gave trudeau salut   \n",
       "10922                wish least marin le pen vote canada   \n",
       "10923                    like choic white genocid candid   \n",
       "10924  white peopl use say sex sin use mysteri saw ch...   \n",
       "10925                                                get   \n",
       "\n",
       "                                              lemmatized  sentiment_score  \\\n",
       "0                 march booklet downloaded time counting           0.0000   \n",
       "1      order help increase booklet downloads would gr...           1.0268   \n",
       "2      simply copy paste following text youtube video...           0.0000   \n",
       "3      click free download colorfully illustrated pag...          -0.0161   \n",
       "4                    click download mb green banner link           0.0000   \n",
       "...                                                  ...              ...   \n",
       "10921  billy guy would n leave alone gave trudeau salute           0.0000   \n",
       "10922               wish least marine le pen vote canada           0.4019   \n",
       "10923               like choice white genocide candidate           0.3612   \n",
       "10924  white people used say sex sin used mystery saw...          -0.5574   \n",
       "10925                                                get           0.0000   \n",
       "\n",
       "                                                  tokens  \\\n",
       "0          [march, booklet, downloaded, times, counting]   \n",
       "1      [order, help, increase, booklets, downloads, w...   \n",
       "2      [simply, copy, paste, following, text, youtube...   \n",
       "3      [click, free, download, colorfully, illustrate...   \n",
       "4             [click, download, mb, green, banner, link]   \n",
       "...                                                  ...   \n",
       "10921  [billy, guy, would, n, leave, alone, gave, tru...   \n",
       "10922       [wish, least, marine, le, pen, vote, canada]   \n",
       "10923        [like, choices, white, genocide, candidate]   \n",
       "10924  [white, people, used, say, sex, sin, used, mys...   \n",
       "10925                                              [get]   \n",
       "\n",
       "                                                     POS  \\\n",
       "0      [(march, NN), (booklet, NN), (downloaded, VBD)...   \n",
       "1      [(order, NN), (help, NN), (increase, VB), (boo...   \n",
       "2      [(simply, RB), (copy, JJ), (paste, NN), (follo...   \n",
       "3      [(click, JJ), (free, JJ), (download, NN), (col...   \n",
       "4      [(click, NN), (download, NN), (mb, NN), (green...   \n",
       "...                                                  ...   \n",
       "10921  [(billy, RB), (guy, NN), (would, MD), (n, VB),...   \n",
       "10922  [(wish, JJ), (least, JJS), (marine, JJ), (le, ...   \n",
       "10923  [(like, IN), (choice, NN), (white, JJ), (genoc...   \n",
       "10924  [(white, JJ), (people, NNS), (used, VBN), (say...   \n",
       "10925                                        [(get, VB)]   \n",
       "\n",
       "                                                     ADJ  \\\n",
       "0                                                     []   \n",
       "1                                          [great, text]   \n",
       "2                                                 [copy]   \n",
       "3      [click, free, illustrated, e, intentional, wes...   \n",
       "4                                                [green]   \n",
       "...                                                  ...   \n",
       "10921                                          [trudeau]   \n",
       "10922                         [wish, least, marine, pen]   \n",
       "10923                                            [white]   \n",
       "10924                       [white, child, brown, mixed]   \n",
       "10925                                                 []   \n",
       "\n",
       "                                                    NOUN  \n",
       "0                       [march, booklet, time, counting]  \n",
       "1      [order, help, booklet, downloads, stormfronter...  \n",
       "2        [paste, text, youtube, video, description, box]  \n",
       "3      [download, page, book, zionist, destruction, c...  \n",
       "4                    [click, download, mb, banner, link]  \n",
       "...                                                  ...  \n",
       "10921                                      [guy, salute]  \n",
       "10922                                 [le, vote, canada]  \n",
       "10923                      [choice, genocide, candidate]  \n",
       "10924           [people, sex, sin, mystery, race, child]  \n",
       "10925                                                 []  \n",
       "\n",
       "[10926 rows x 14 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = data['text_clean'].apply(word_tokenize)\n",
    "nouns = []\n",
    "for sentence in tokens:\n",
    "     for word,pos in nltk.pos_tag(nltk.word_tokenize(str(sentence))):\n",
    "        if (pos == 'NN' or pos == 'NNP' or pos == 'NNS' or pos == 'NNPS'):\n",
    "             nouns.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'march\",\n",
       " \"'booklet\",\n",
       " \"'times\",\n",
       " ']',\n",
       " \"'order\",\n",
       " \"'increase\",\n",
       " \"'booklets\",\n",
       " \"'downloads\",\n",
       " \"'great\",\n",
       " \"'stormfronters\",\n",
       " \"'youtube\",\n",
       " \"'accounts\",\n",
       " \"'description\",\n",
       " \"'boxes\",\n",
       " \"'youtube\",\n",
       " ']',\n",
       " \"'simply\",\n",
       " \"'youtube\",\n",
       " \"'videos\",\n",
       " \"'description\",\n",
       " \"'boxes\",\n",
       " ']',\n",
       " \"'click\",\n",
       " \"'page\",\n",
       " 'e',\n",
       " \"'book\",\n",
       " \"'intentional\",\n",
       " \"'destruction\",\n",
       " \"'western\",\n",
       " \"'civilization\",\n",
       " ']',\n",
       " \"'click\",\n",
       " \"'mb\",\n",
       " \"'banner\",\n",
       " ']',\n",
       " \"'booklet\",\n",
       " ']',\n",
       " \"'downloads\",\n",
       " ']',\n",
       " \"'pdf\",\n",
       " \"'docx\",\n",
       " \"'video\",\n",
       " \"'version\",\n",
       " \"'western\",\n",
       " \"'civilization\",\n",
       " \"'white\",\n",
       " \"'yt\",\n",
       " \"'comment\",\n",
       " \"'txt\",\n",
       " \"'txt\",\n",
       " \"'www\",\n",
       " \"'youtube\",\n",
       " \"'com\",\n",
       " 'v',\n",
       " \"'hg\",\n",
       " \"'alpm\",\n",
       " \"'booklet\",\n",
       " \"'link\",\n",
       " \"'world\",\n",
       " ']',\n",
       " \"'simply\",\n",
       " \"'links\",\n",
       " \"'paste\",\n",
       " \"'description\",\n",
       " \"'box\",\n",
       " \"'youtube\",\n",
       " ']',\n",
       " \"'thank\",\n",
       " \"'advance\",\n",
       " \"'youtube\",\n",
       " \"'description\",\n",
       " \"'box\",\n",
       " \"'info\",\n",
       " \"'info\",\n",
       " ']',\n",
       " '[',\n",
       " \"'may\",\n",
       " \"'jew\",\n",
       " \"'stupid\",\n",
       " \"'seems\",\n",
       " \"'blacks\",\n",
       " \"'wo\",\n",
       " \"'kill\",\n",
       " \"'alongside\",\n",
       " \"'every\",\n",
       " \"'white\",\n",
       " \"'get\",\n",
       " ']',\n",
       " \"'thank\",\n",
       " \"'info\",\n",
       " ']',\n",
       " \"'hope\",\n",
       " \"'room\",\n",
       " ']',\n",
       " \"'came\",\n",
       " \"'piece\",\n",
       " \"'garbage\",\n",
       " ']',\n",
       " \"'comments\",\n",
       " \"'say\",\n",
       " ']',\n",
       " \"'graphic\",\n",
       " ']',\n",
       " '[',\n",
       " \"'die\",\n",
       " \"'antwoord\",\n",
       " \"'cookie\",\n",
       " \"'thumper\",\n",
       " \"'official\",\n",
       " \"'video\",\n",
       " ']',\n",
       " '[',\n",
       " \"'whites\",\n",
       " \"'range\",\n",
       " \"'million\",\n",
       " ']',\n",
       " \"'believe\",\n",
       " \"'since\",\n",
       " \"'reports\",\n",
       " \"'whites\",\n",
       " \"'state\",\n",
       " \"'census\",\n",
       " ']',\n",
       " \"'thank\",\n",
       " \"'story\",\n",
       " ']',\n",
       " \"'think\",\n",
       " \"'write\",\n",
       " \"'book\",\n",
       " \"'well\",\n",
       " ']',\n",
       " '[',\n",
       " \"'always\",\n",
       " \"'professions\",\n",
       " \"'get\",\n",
       " \"'neighbourhood\",\n",
       " \"'kids\",\n",
       " ']',\n",
       " \"'sad\",\n",
       " \"'white\",\n",
       " \"'students\",\n",
       " \"'schools\",\n",
       " \"'act\",\n",
       " ']',\n",
       " \"'guess\",\n",
       " \"'stick\",\n",
       " \"'kids\",\n",
       " ']',\n",
       " \"'randy\",\n",
       " \"'blazak\",\n",
       " \"'expert\",\n",
       " \"'right\",\n",
       " \"'teaches\",\n",
       " \"'sociology\",\n",
       " \"'star\",\n",
       " \"'trek\",\n",
       " \"'class\",\n",
       " \"'students\",\n",
       " ']',\n",
       " \"'wonder\",\n",
       " \"'young\",\n",
       " \"'ca\",\n",
       " \"'get\",\n",
       " \"'jobs\",\n",
       " ']',\n",
       " \"'take\",\n",
       " \"'underwater\",\n",
       " \"'basket\",\n",
       " \"'courses\",\n",
       " \"'skills\",\n",
       " \"'account\",\n",
       " \"'angry\",\n",
       " \"'still\",\n",
       " \"'favorite\",\n",
       " \"'comment\",\n",
       " ']',\n",
       " \"'tarzan\",\n",
       " \"'movies\",\n",
       " \"'zulu\",\n",
       " \"'kong\",\n",
       " \"'images\",\n",
       " ']',\n",
       " \"'national\",\n",
       " \"'geographic\",\n",
       " \"'magazines\",\n",
       " \"'replete\",\n",
       " \"'color\",\n",
       " \"'photos\",\n",
       " \"'villagers\",\n",
       " ']',\n",
       " \"'dr\",\n",
       " \"'albert\",\n",
       " \"'schweitzer\",\n",
       " \"'two\",\n",
       " \"'documentary\",\n",
       " \"'movie\",\n",
       " \"'theaters\",\n",
       " ']',\n",
       " \"'think\",\n",
       " \"'vhs\",\n",
       " \"'tapes\",\n",
       " \"'amazon\",\n",
       " ']',\n",
       " \"'gods\",\n",
       " \"'pale\",\n",
       " \"'skin\",\n",
       " \"'blue\",\n",
       " \"'eyes\",\n",
       " \"'suppose\",\n",
       " \"'literacy\",\n",
       " \"'cultural\",\n",
       " ']',\n",
       " \"'girl\",\n",
       " \"'starts\",\n",
       " \"'guy\",\n",
       " \"'middles\",\n",
       " \"'lips\",\n",
       " \"'start\",\n",
       " ']',\n",
       " \"'lines\",\n",
       " \"'bad\",\n",
       " \"'actors\",\n",
       " \"'ca\",\n",
       " ']',\n",
       " \"'whole\",\n",
       " \"'situation\",\n",
       " \"'starts\",\n",
       " \"'become\",\n",
       " ']',\n",
       " \"'sector\",\n",
       " \"'supporter\",\n",
       " \"'starts\",\n",
       " \"'fact\",\n",
       " \"'agents\",\n",
       " ']',\n",
       " '[',\n",
       " 'n',\n",
       " \"'know\",\n",
       " \"'support\",\n",
       " ']',\n",
       " \"'israel\",\n",
       " \"'gives\",\n",
       " \"'us\",\n",
       " \"'law\",\n",
       " \"'enforcement\",\n",
       " ']',\n",
       " \"'remember\",\n",
       " \"'boston\",\n",
       " \"'marathon\",\n",
       " \"'bombings\",\n",
       " \"'fox\",\n",
       " \"'news\",\n",
       " \"'said\",\n",
       " \"'law\",\n",
       " \"'enforcement\",\n",
       " \"'israel\",\n",
       " \"'advice\",\n",
       " ']',\n",
       " \"'believe\",\n",
       " \"'dog\",\n",
       " \"'get\",\n",
       " \"'far\",\n",
       " \"'away\",\n",
       " ']',\n",
       " \"'carolina\",\n",
       " \"'peterson\",\n",
       " \"'particulars\",\n",
       " ']',\n",
       " \"'section\",\n",
       " 'c',\n",
       " \"'jobs\",\n",
       " \"'areas\",\n",
       " \"'well\",\n",
       " \"'thanks\",\n",
       " ']',\n",
       " \"'greece\",\n",
       " \"'man\",\n",
       " \"'adolf\",\n",
       " \"'hitler\",\n",
       " \"'perfect\",\n",
       " \"'time\",\n",
       " \"'start\",\n",
       " \"'national\",\n",
       " \"'movement\",\n",
       " \"'country\",\n",
       " \"'bankers\",\n",
       " ']',\n",
       " \"'image\",\n",
       " \"'first\",\n",
       " \"'march\",\n",
       " \"'freeman\",\n",
       " \"'court\",\n",
       " \"'judge\",\n",
       " \"'bows\",\n",
       " \"'sovereign\",\n",
       " \"'canada\",\n",
       " \"'youtube\",\n",
       " \"'freeman\",\n",
       " \"'court\",\n",
       " \"'judge\",\n",
       " \"'bows\",\n",
       " \"'sovereign\",\n",
       " \"'canada\",\n",
       " ']',\n",
       " '[',\n",
       " 'n',\n",
       " 'v',\n",
       " \"'hollywood\",\n",
       " \"'movies\",\n",
       " ']',\n",
       " \"'know\",\n",
       " \"'chris\",\n",
       " \"'rock\",\n",
       " \"'long\",\n",
       " ']',\n",
       " \"'little\",\n",
       " ']',\n",
       " \"'war\",\n",
       " \"'innocent\",\n",
       " \"'white\",\n",
       " \"'population\",\n",
       " \"'nation\",\n",
       " \"'white\",\n",
       " ']',\n",
       " \"'seekers\",\n",
       " \"'truth\",\n",
       " \"'start\",\n",
       " \"'benjamin\",\n",
       " \"'freedman\",\n",
       " \"'speaks\",\n",
       " \"'video\",\n",
       " ']',\n",
       " \"'com\",\n",
       " 'v',\n",
       " \"'hxbcyx\",\n",
       " \"'jgm\",\n",
       " \"'move\",\n",
       " \"'film\",\n",
       " ']',\n",
       " \"'com\",\n",
       " 'v',\n",
       " \"'kbd\",\n",
       " \"'gm\",\n",
       " \"'vodm\",\n",
       " \"'min\",\n",
       " \"'obama\",\n",
       " \"'aipac\",\n",
       " ']',\n",
       " \"'com\",\n",
       " 'v',\n",
       " \"'krdycxpb\",\n",
       " \"'fs\",\n",
       " \"'entire\",\n",
       " \"'myron\",\n",
       " \"'fagan\",\n",
       " \"'video\",\n",
       " ']',\n",
       " \"'com\",\n",
       " 'v',\n",
       " \"'ok\",\n",
       " \"'ks\",\n",
       " \"'bsoy\",\n",
       " \"'booklet\",\n",
       " \"'video\",\n",
       " \"'intentional\",\n",
       " \"'destruction\",\n",
       " \"'white\",\n",
       " \"'go\",\n",
       " ']',\n",
       " \"'com\",\n",
       " 'v',\n",
       " ']',\n",
       " ']',\n",
       " \"'wntube\",\n",
       " \"'net\",\n",
       " \"'com\",\n",
       " ']',\n",
       " \"'site\",\n",
       " ']',\n",
       " \"'please\",\n",
       " \"'donate\",\n",
       " ']',\n",
       " '[',\n",
       " \"'code\",\n",
       " ']',\n",
       " \"'try\",\n",
       " \"'el\",\n",
       " \"'paso\",\n",
       " \"'texas\",\n",
       " \"'soo\",\n",
       " \"'mudd\",\n",
       " \"'invasion\",\n",
       " \"'color\",\n",
       " ']',\n",
       " \"'welcome\",\n",
       " \"'front\",\n",
       " \"'lines\",\n",
       " ']',\n",
       " \"'folk\",\n",
       " \"'faith\",\n",
       " \"'hail\",\n",
       " \"'victory\",\n",
       " ']',\n",
       " \"'video\",\n",
       " \"'event\",\n",
       " \"'see\",\n",
       " \"'white\",\n",
       " \"'scum\",\n",
       " \"'scum\",\n",
       " \"'scum\",\n",
       " 'f',\n",
       " \"'scum\",\n",
       " \"'muslim\",\n",
       " \"'filth\",\n",
       " ']',\n",
       " \"'protesters\",\n",
       " \"'clash\",\n",
       " \"'military\",\n",
       " ']',\n",
       " \"'show\",\n",
       " \"'stupid\",\n",
       " \"'folks\",\n",
       " ']',\n",
       " \"'let\",\n",
       " \"'pizza\",\n",
       " \"'try\",\n",
       " \"'assault\",\n",
       " \"'know\",\n",
       " ']',\n",
       " \"'nice\",\n",
       " \"'know\",\n",
       " \"'still\",\n",
       " \"'men\",\n",
       " \"'pair\",\n",
       " ']',\n",
       " \"'courage\",\n",
       " ']',\n",
       " \"'jesus\",\n",
       " \"'lord\",\n",
       " ']',\n",
       " \"'nice\",\n",
       " ']',\n",
       " \"'male\",\n",
       " \"'cow\",\n",
       " \"'female\",\n",
       " \"'cow\",\n",
       " \"'process\",\n",
       " \"'understand\",\n",
       " ']',\n",
       " '[',\n",
       " \"'get\",\n",
       " \"'mad\",\n",
       " \"'cow\",\n",
       " \"'disease\",\n",
       " ']',\n",
       " \"'sooner\",\n",
       " \"'better\",\n",
       " ']',\n",
       " \"'yeah\",\n",
       " ']',\n",
       " \"'kids\",\n",
       " \"'church\",\n",
       " \"'years\",\n",
       " ']',\n",
       " ']',\n",
       " \"'sa\",\n",
       " \"'threads\",\n",
       " ']',\n",
       " \"'white\",\n",
       " \"'folks\",\n",
       " \"'require\",\n",
       " \"'little\",\n",
       " ']',\n",
       " \"'guess\",\n",
       " \"'mr\",\n",
       " \"'pena\",\n",
       " \"'forgot\",\n",
       " \"'skater\",\n",
       " \"'nbc\",\n",
       " \"'every\",\n",
       " \"'minutes\",\n",
       " ']',\n",
       " \"'sad\",\n",
       " \"'right\",\n",
       " \"'also\",\n",
       " \"'sorry\",\n",
       " \"'kevin\",\n",
       " \"'several\",\n",
       " \"'years\",\n",
       " ']',\n",
       " '[',\n",
       " ']',\n",
       " \"'hard\",\n",
       " \"'time\",\n",
       " ']',\n",
       " \"'friends\",\n",
       " \"'cs\",\n",
       " ']',\n",
       " \"'coworkers\",\n",
       " \"'white\",\n",
       " \"'ne\",\n",
       " \"'paki\",\n",
       " \"'guy\",\n",
       " \"'mestizo\",\n",
       " ']',\n",
       " \"'hundreds\",\n",
       " \"'blacks\",\n",
       " ']',\n",
       " '[',\n",
       " \"'masters\",\n",
       " \"'phd\",\n",
       " \"'graduation\",\n",
       " \"'ceremony\",\n",
       " \"'state\",\n",
       " \"'university\",\n",
       " \"'years\",\n",
       " \"'cs\",\n",
       " \"'degrees\",\n",
       " \"'blacks\",\n",
       " \"'computer\",\n",
       " \"'degrees\",\n",
       " \"'went\",\n",
       " \"'blacks\",\n",
       " \"'focus\",\n",
       " \"'internet\",\n",
       " \"'web\",\n",
       " \"'commerce\",\n",
       " \"'computer\",\n",
       " \"'geniuses\",\n",
       " ']',\n",
       " \"'home\",\n",
       " \"'kids\",\n",
       " \"'public\",\n",
       " \"'school\",\n",
       " \"'kids\",\n",
       " \"'bees\",\n",
       " ']',\n",
       " \"'poor\",\n",
       " \"'white\",\n",
       " \"'families\",\n",
       " \"'parents\",\n",
       " ']',\n",
       " \"'perhaps\",\n",
       " \"'school\",\n",
       " \"'district\",\n",
       " \"'white\",\n",
       " \"'families\",\n",
       " \"'get\",\n",
       " \"'parent\",\n",
       " \"'teach\",\n",
       " \"'kids\",\n",
       " \"'per\",\n",
       " ']',\n",
       " \"'parents\",\n",
       " \"'teach\",\n",
       " \"'weekdays\",\n",
       " \"'teach\",\n",
       " ']',\n",
       " '[',\n",
       " ']',\n",
       " \"'intention\",\n",
       " \"'foot\",\n",
       " \"'mexico\",\n",
       " \"'take\",\n",
       " \"'word\",\n",
       " ']',\n",
       " \"'excellent\",\n",
       " \"'article\",\n",
       " \"'jail\",\n",
       " ']',\n",
       " \"'suspects\",\n",
       " \"'death\",\n",
       " \"'county\",\n",
       " \"'girl\",\n",
       " \"'repeat\",\n",
       " \"'offenders\",\n",
       " \"'probation\",\n",
       " ']',\n",
       " \"'steve\",\n",
       " \"'reporter\",\n",
       " \"'suspects\",\n",
       " \"'county\",\n",
       " \"'girl\",\n",
       " \"'sentences\",\n",
       " \"'crimes\",\n",
       " \"'probation\",\n",
       " \"'time\",\n",
       " ']',\n",
       " '[',\n",
       " \"'perry\",\n",
       " \"'schiro\",\n",
       " \"'harnett\",\n",
       " \"'death\",\n",
       " \"'elizabeth\",\n",
       " \"'haddock\",\n",
       " ']',\n",
       " \"'fourth\",\n",
       " \"'suspect\",\n",
       " ']',\n",
       " '[',\n",
       " \"'sherrod\",\n",
       " \"'nicholas\",\n",
       " \"'harrison\",\n",
       " \"'michael\",\n",
       " \"'graham\",\n",
       " \"'currie\",\n",
       " \"'van\",\n",
       " \"'roger\",\n",
       " \"'smith\",\n",
       " \"'cameron\",\n",
       " \"'harnett\",\n",
       " ']',\n",
       " \"'two\",\n",
       " \"'suspects\",\n",
       " \"'mr\",\n",
       " \"'harrison\",\n",
       " \"'mr\",\n",
       " \"'currie\",\n",
       " \"'repeat\",\n",
       " \"'probation\",\n",
       " \"'violators\",\n",
       " ']',\n",
       " \"'mr\",\n",
       " \"'smith\",\n",
       " \"'prior\",\n",
       " \"'criminal\",\n",
       " \"'record\",\n",
       " ']',\n",
       " \"'mr\",\n",
       " \"'schiro\",\n",
       " \"'suspect\",\n",
       " \"'caliber\",\n",
       " \"'handgun\",\n",
       " ']',\n",
       " '[',\n",
       " \"'burglary\",\n",
       " \"'larceny\",\n",
       " \"'possession\",\n",
       " \"'stolen\",\n",
       " ']',\n",
       " \"'suspects\",\n",
       " \"'held\",\n",
       " \"'county\",\n",
       " \"'jail\",\n",
       " ']',\n",
       " \"'story\",\n",
       " ']',\n",
       " \"'nevermind\",\n",
       " \"'new\",\n",
       " \"'page\",\n",
       " \"'knew\",\n",
       " \"'report\",\n",
       " ']',\n",
       " '[',\n",
       " ']',\n",
       " '[',\n",
       " ']',\n",
       " \"'obama\",\n",
       " \"'prince\",\n",
       " \"'blacks\",\n",
       " \"'lodge\",\n",
       " \"'become\",\n",
       " \"'president\",\n",
       " \"'guy\",\n",
       " \"'youtube\",\n",
       " 'v',\n",
       " \"'nlfrsregii\",\n",
       " \"'zagami\",\n",
       " ']',\n",
       " \"'act\",\n",
       " \"'negros\",\n",
       " \"'aids\",\n",
       " \"'right\",\n",
       " \"'reproduce\",\n",
       " \"'die\",\n",
       " \"'way\",\n",
       " \"'take\",\n",
       " \"'care\",\n",
       " \"'kid\",\n",
       " \"'cant\",\n",
       " \"'believe\",\n",
       " \"'millions\",\n",
       " ']',\n",
       " ']',\n",
       " \"'hope\",\n",
       " ']',\n",
       " \"'white\",\n",
       " \"'person\",\n",
       " \"'single\",\n",
       " \"'penny\",\n",
       " \"'single\",\n",
       " \"'finger\",\n",
       " ']',\n",
       " \"'feral\",\n",
       " \"'negro\",\n",
       " \"'yes\",\n",
       " \"'feral\",\n",
       " \"'humans\",\n",
       " \"'not\",\n",
       " \"'pop\",\n",
       " \"'children\",\n",
       " ']',\n",
       " \"'walks\",\n",
       " \"'upright\",\n",
       " \"'human\",\n",
       " \"'arms\",\n",
       " ']',\n",
       " \"'negroes\",\n",
       " \"'look\",\n",
       " \"'act\",\n",
       " ']',\n",
       " ']',\n",
       " \"'every\",\n",
       " \"'time\",\n",
       " \"'see\",\n",
       " \"'comment\",\n",
       " \"'georgia\",\n",
       " \"'facts\",\n",
       " ']',\n",
       " \"'respond\",\n",
       " \"'scum\",\n",
       " ']',\n",
       " \"'reason\",\n",
       " \"'white\",\n",
       " \"'families\",\n",
       " \"'days\",\n",
       " \"'build\",\n",
       " \"'population\",\n",
       " \"'white\",\n",
       " \"'germans\",\n",
       " \"'attacks\",\n",
       " \"'ww\",\n",
       " \"'hyperinflation\",\n",
       " ']',\n",
       " \"'see\",\n",
       " \"'nation\",\n",
       " \"'front\",\n",
       " \"'door\",\n",
       " \"'feel\",\n",
       " \"'minority\",\n",
       " ']',\n",
       " \"'white\",\n",
       " \"'woman\",\n",
       " \"'go\",\n",
       " \"'public\",\n",
       " \"'assault\",\n",
       " ']',\n",
       " \"'see\",\n",
       " ']',\n",
       " \"'hope\",\n",
       " \"'say\",\n",
       " \"'take\",\n",
       " \"'country\",\n",
       " ']',\n",
       " \"'btl\",\n",
       " ']',\n",
       " \"'ok\",\n",
       " \"'threads\",\n",
       " \"'section\",\n",
       " \"'ns\",\n",
       " \"'videos\",\n",
       " ']',\n",
       " \"'misunderstood\",\n",
       " ']',\n",
       " \"'retract\",\n",
       " \"'always\",\n",
       " \"'always\",\n",
       " \"'animal\",\n",
       " \"'lacks\",\n",
       " \"'skills\",\n",
       " \"'raise\",\n",
       " \"'human\",\n",
       " ']',\n",
       " \"'apologize\",\n",
       " \"'ignorance\",\n",
       " ']',\n",
       " \"'baby\",\n",
       " \"'steps\",\n",
       " ']',\n",
       " '[',\n",
       " 'n',\n",
       " \"'think\",\n",
       " \"'non\",\n",
       " \"'whites\",\n",
       " \"'white\",\n",
       " \"'nations\",\n",
       " ']',\n",
       " '[',\n",
       " \"'whites\",\n",
       " \"'white\",\n",
       " \"'laid\",\n",
       " \"'feel\",\n",
       " \"'act\",\n",
       " \"'relations\",\n",
       " \"'children\",\n",
       " \"'blacks\",\n",
       " \"'problem\",\n",
       " \"'white\",\n",
       " \"'race\",\n",
       " ']',\n",
       " \"'people\",\n",
       " \"'idea\",\n",
       " \"'knowledge\",\n",
       " ']',\n",
       " '[',\n",
       " \"'whites\",\n",
       " \"'knowledge\",\n",
       " \"'little\",\n",
       " \"'recognition\",\n",
       " \"'put\",\n",
       " \"'feel\",\n",
       " \"'little\",\n",
       " \"'pride\",\n",
       " \"'stick\",\n",
       " \"'white\",\n",
       " \"'race\",\n",
       " ']',\n",
       " \"'problems\",\n",
       " \"'different\",\n",
       " \"'classes\",\n",
       " \"'inside\",\n",
       " \"'race\",\n",
       " \"'white\",\n",
       " \"'since\",\n",
       " \"'fight\",\n",
       " \"'non\",\n",
       " \"'whites\",\n",
       " \"'enemies\",\n",
       " ']',\n",
       " \"'person\",\n",
       " \"'white\",\n",
       " \"'pagan\",\n",
       " \"'sister\",\n",
       " \"'matter\",\n",
       " \"'profession\",\n",
       " ']',\n",
       " \"'examples\",\n",
       " \"'progress\",\n",
       " \"'skinheads\",\n",
       " ']',\n",
       " \"'seems\",\n",
       " \"'decline\",\n",
       " ']',\n",
       " \"'years\",\n",
       " \"'class\",\n",
       " ']',\n",
       " \"'dont\",\n",
       " \"'create\",\n",
       " \"'division\",\n",
       " ']',\n",
       " \"'black\",\n",
       " \"'women\",\n",
       " \"'london\",\n",
       " \"'transport\",\n",
       " ']',\n",
       " \"'racist\",\n",
       " \"'woman\",\n",
       " \"'rant\",\n",
       " \"'london\",\n",
       " \"'part\",\n",
       " \"'youtube\",\n",
       " \"'original\",\n",
       " \"'girls\",\n",
       " \"'south\",\n",
       " \"'london\",\n",
       " \"'railway\",\n",
       " \"'balham\",\n",
       " \"'mitcham\",\n",
       " ']',\n",
       " \"'quote\",\n",
       " \"'south\",\n",
       " \"'chocolate\",\n",
       " \"'overseas\",\n",
       " \"'companies\",\n",
       " \"'sa\",\n",
       " \"'mars\",\n",
       " \"'bar\",\n",
       " \"'taste\",\n",
       " \"'different\",\n",
       " \"'sa\",\n",
       " \"'england\",\n",
       " \"'thought\",\n",
       " \"'stuff\",\n",
       " \"'tastes\",\n",
       " \"'different\",\n",
       " ']',\n",
       " \"'think\",\n",
       " \"'sugar\",\n",
       " \"'sugar\",\n",
       " ']',\n",
       " \"'btw\",\n",
       " \"'mars\",\n",
       " ']',\n",
       " \"'drink\",\n",
       " \"'diet\",\n",
       " \"'water\",\n",
       " ']',\n",
       " \"'try\",\n",
       " \"'gmo\",\n",
       " \"'stuff\",\n",
       " \"'since\",\n",
       " \"'label\",\n",
       " \"'stuff\",\n",
       " \"'hard\",\n",
       " ']',\n",
       " \"'high\",\n",
       " \"'fructose\",\n",
       " \"'syrup\",\n",
       " \"'bad\",\n",
       " \"'gm\",\n",
       " ']',\n",
       " '[',\n",
       " \"'soya\",\n",
       " ']',\n",
       " \"'pesticides\",\n",
       " \"'fresh\",\n",
       " \"'fruit\",\n",
       " \"'veggies\",\n",
       " ']',\n",
       " '[',\n",
       " \"'chemicals\",\n",
       " \"'sometimes\",\n",
       " \"'food\",\n",
       " ']',\n",
       " \"'end\",\n",
       " \"'safe\",\n",
       " \"'eat\",\n",
       " \"'grass\",\n",
       " \"'lucky\",\n",
       " \"'garden\",\n",
       " \"'food\",\n",
       " \"'space\",\n",
       " \"'hell\",\n",
       " \"'government\",\n",
       " ']',\n",
       " \"'hell\",\n",
       " \"'way\",\n",
       " ']',\n",
       " '[',\n",
       " \"'check\",\n",
       " \"'may\",\n",
       " \"'planet\",\n",
       " \"'apes\",\n",
       " ']',\n",
       " \"'youtube\",\n",
       " 'x',\n",
       " \"'kenya\",\n",
       " \"'death\",\n",
       " \"'panga\",\n",
       " \"'front\",\n",
       " \"'film\",\n",
       " \"'crew\",\n",
       " 'x',\n",
       " 'c',\n",
       " ']',\n",
       " \"'racist\",\n",
       " \"'blacks\",\n",
       " \"'young\",\n",
       " \"'white\",\n",
       " \"'girl\",\n",
       " \"'france\",\n",
       " ']',\n",
       " \"'negro\",\n",
       " \"'laughs\",\n",
       " \"'glee\",\n",
       " \"'dailymotion\",\n",
       " \"'agression\",\n",
       " \"'raciste\",\n",
       " \"'contre\",\n",
       " \"'une\",\n",
       " \"'jeune\",\n",
       " \"'fille\",\n",
       " \"'blanche\",\n",
       " \"'france\",\n",
       " \"'une\",\n",
       " \"'vid\",\n",
       " \"'news\",\n",
       " \"'politics\",\n",
       " ']',\n",
       " '[',\n",
       " \"'happens\",\n",
       " \"'popeye\",\n",
       " \"'runs\",\n",
       " \"'youtube\",\n",
       " \"'popeyes\",\n",
       " \"'ghetto\",\n",
       " \"'vers\",\n",
       " ']',\n",
       " \"'alot\",\n",
       " \"'profanity\",\n",
       " ']',\n",
       " \"'negro\",\n",
       " ']',\n",
       " \"'click\",\n",
       " \"'video\",\n",
       " \"'video\",\n",
       " \"'comments\",\n",
       " \"'full\",\n",
       " \"'youtube\",\n",
       " \"'baka\",\n",
       " ...]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('0', 'CD'), ('march', 'JJ'), ('booklet', 'NN'), ('downloaded', 'VBD'), ('times', 'NNS'), ('counting', 'VBG'), ('1', 'CD'), ('order', 'NN'), ('help', 'NN'), ('increase', 'VB'), ('booklets', 'NNS'), ('downloads', 'NNS'), ('would', 'MD'), ('g', 'VB'), ('...', ':'), ('2', 'CD'), ('simply', 'RB'), ('copy', 'JJ'), ('paste', 'NN'), ('following', 'VBG'), ('text', 'NN'), ('youtube', 'NN'), ('video', 'NN'), ('...', ':'), ('3', 'CD'), ('click', 'JJ'), ('free', 'JJ'), ('download', 'NN'), ('colorfully', 'RB'), ('illustrated', 'VBD'), ('pag', 'NN'), ('...', ':'), ('4', 'CD'), ('click', 'NN'), ('download', 'NN'), ('mb', 'NN'), ('green', 'JJ'), ('banner', 'NN'), ('link', 'NN'), ('...', ':'), ('10921', 'CD'), ('billy', 'RB'), ('guy', 'NN'), ('would', 'MD'), ('n', 'VB'), ('leave', 'VB'), ('alone', 'RB'), ('gave', 'VBD'), ('trudeau', 'JJ'), ('salute', 'NN'), ('10922', 'CD'), ('wish', 'JJ'), ('least', 'JJS'), ('marine', 'JJ'), ('le', 'NN'), ('pen', 'JJ'), ('vote', 'NN'), ('canada', 'VBD'), ('10923', 'CD'), ('like', 'IN'), ('choices', 'NNS'), ('white', 'JJ'), ('genocide', 'JJ'), ('candidate', 'NN'), ('10924', 'CD'), ('white', 'JJ'), ('people', 'NNS'), ('used', 'VBN'), ('say', 'VBP'), ('sex', 'NN'), ('sin', 'NN'), ('used', 'VBN'), ('mystery', 'NN'), ('saw', 'VBD'), ('...', ':'), ('10925', 'CD'), ('get', 'NN'), ('Name', 'NN'), (':', ':'), ('text_clean', 'NN'), (',', ','), ('Length', 'NNP'), (':', ':'), ('10926', 'CD'), (',', ','), ('dtype', 'NN'), (':', ':'), ('object', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(str(data['text_clean']))\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "print(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              As of March 13th , 2014 , the booklet had been downloaded over 18,300 times and counting .\n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       In order to help increase the booklets downloads , it would be great if all Stormfronters who had YouTube accounts , could display the following text in the description boxes of their uploaded YouTube videos .\n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                ( Simply copy and paste the following text into your YouTube videos description boxes. )\n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Click below for a FREE download of a colorfully illustrated 132 page e-book on the Zionist-engineered INTENTIONAL destruction of Western civilization .\n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Click on the `` DOWNLOAD ( 7.42 MB ) '' green banner link .\n",
       "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Booklet updated on Feb. 14th , 2014 .\n",
       "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     ( Now with over 18,300 Downloads. )\n",
       "7    PDF file : http://www.mediafire.com/download/7p3p3goadvvqvsf/WNDebateBooklet_2-14-14.pdfMSWordfile:http://www.mediafire.com/download/psezkkk4d6a3wt1/WNDebateBooklet _ 2-14-14.docx Watch the 10 hour video version of `` The Zionist Attack on Western Civilization '' @ http://trutube.tv/video/14247/The-Zionist-Attack-on-Western-Civilization-Pages-1-33-Part-1-of-4-Banned-from-YouTubeNotepadPromotionalYouTubeComment:http://www.mediafire.com/download/96fg6ft02lyfruz/Booklet _ White _ YT _ Comment _ ( Hyperlinked ) .txt http://www.mediafire.com/download/zcn3wozjbwnezms/Booklet-White-YT-Comment- ( Hyperlinked-Back-Up % 5D.txt http://www.mediafire.com/download/9uyudq1yuxu1dur/Booklet+Comment+%28Firefox%29.txt2minutepromotionalBOOKLETvideo@http://www.youtube.com/watch ? v = _ 8hg254ALpM Are you interested in helping spread the booklet download link across the world ?\n",
       "8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Then why not simply copy this text ( & links ) and paste it into the description box of your YouTube videos ?\n",
       "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Thank you in advance. : ) Download the youtube `` description box '' info text file below @ http://www.mediafire.com/download/dqhn1czprr17o21/Booklet-Description-Box _ Info.txt\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "data['text'][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label (0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1428, 14)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[data['label'] == 1].shape # 1428 over 10926 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "hate = data.loc[data['label'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "white 400\n",
      "black 217\n",
      "like 132\n",
      "jew 117\n",
      "people 106\n",
      "would 94\n",
      "negro 94\n",
      "race 88\n",
      "one 87\n",
      "get 86\n",
      "country 71\n",
      "see 71\n",
      "think 65\n",
      "go 63\n",
      "non 58\n",
      "want 56\n",
      "back 53\n",
      "time 52\n",
      "school 49\n",
      "woman 48\n",
      "even 48\n",
      "make 47\n",
      "asian 47\n",
      "look 46\n",
      "know 46\n",
      "thing 45\n",
      "way 45\n",
      "kid 44\n",
      "say 44\n",
      "good 44\n",
      "every 43\n",
      "day 42\n",
      "never 41\n",
      "child 41\n",
      "need 40\n",
      "come 38\n",
      "africa 38\n",
      "liberal 37\n",
      "ape 36\n",
      "world 36\n",
      "many 36\n",
      "around 36\n",
      "going 35\n",
      "youtube 33\n",
      "year 32\n",
      "last 32\n",
      "got 32\n",
      "find 32\n",
      "much 31\n",
      "scum 31\n"
     ]
    }
   ],
   "source": [
    "common_words = get_top_n_words(hate['lemmatized'], 50)\n",
    "for word, freq in common_words:\n",
    "    print(word,freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['white',\n",
       " 'black',\n",
       " 'n',\n",
       " 'u',\n",
       " 'good',\n",
       " 'asian',\n",
       " 'non',\n",
       " 'many',\n",
       " 'liberal',\n",
       " 'negro',\n",
       " 'last',\n",
       " 'little',\n",
       " 'much',\n",
       " 'new',\n",
       " 'great',\n",
       " 'jewish',\n",
       " 'live',\n",
       " 'bad',\n",
       " 'old',\n",
       " 'irish']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj1 = hate['lemmatized'].apply(get_adjectives)\n",
    "adj_list1 = [j for i in adj1 for j in i]\n",
    "adj_counter1 = {}\n",
    "for word in adj_list1:\n",
    "    if word in adj_counter1:\n",
    "        adj_counter1[word] += 1\n",
    "    else:\n",
    "        adj_counter1[word] = 1\n",
    "            \n",
    "popular_adj1 = sorted(adj_counter1, key = adj_counter1.get, reverse = True)\n",
    "top_20_adj1 = popular_adj1[:20]\n",
    "top_20_adj1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['people',\n",
       " 'jew',\n",
       " 'race',\n",
       " 'country',\n",
       " 'time',\n",
       " 'school',\n",
       " 'woman',\n",
       " 'thing',\n",
       " 'way',\n",
       " 'day',\n",
       " 'negro',\n",
       " 'world',\n",
       " 'kid',\n",
       " 'ape',\n",
       " 'year',\n",
       " 'child',\n",
       " 'n',\n",
       " 'youtube',\n",
       " 'place',\n",
       " 'home']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noun1 = hate['lemmatized'].apply(get_noun)\n",
    "noun_list1 = [j for i in noun1 for j in i]\n",
    "noun_counter1 = {}\n",
    "for word in noun_list1:\n",
    "    if word in noun_counter1:\n",
    "        noun_counter1[word] += 1\n",
    "    else:\n",
    "        noun_counter1[word] = 1\n",
    "            \n",
    "popular_noun1 = sorted(noun_counter1, key = noun_counter1.get, reverse = True)\n",
    "top_20_noun1 = popular_noun1[:20]\n",
    "top_20_noun1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total</th>\n",
       "      <th>Hate Speech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>white</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>black</td>\n",
       "      <td>black</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>good</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>new</td>\n",
       "      <td>u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>u</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>n</td>\n",
       "      <td>asian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>many</td>\n",
       "      <td>non</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>old</td>\n",
       "      <td>many</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>great</td>\n",
       "      <td>liberal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>last</td>\n",
       "      <td>negro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>live</td>\n",
       "      <td>last</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>much</td>\n",
       "      <td>little</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>little</td>\n",
       "      <td>much</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>american</td>\n",
       "      <td>new</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>free</td>\n",
       "      <td>great</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>asian</td>\n",
       "      <td>jewish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>bad</td>\n",
       "      <td>live</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>irish</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>non</td>\n",
       "      <td>old</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>sure</td>\n",
       "      <td>irish</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Total Hate Speech\n",
       "0      white       white\n",
       "1      black       black\n",
       "2       good           n\n",
       "3        new           u\n",
       "4          u        good\n",
       "5          n       asian\n",
       "6       many         non\n",
       "7        old        many\n",
       "8      great     liberal\n",
       "9       last       negro\n",
       "10      live        last\n",
       "11      much      little\n",
       "12    little        much\n",
       "13  american         new\n",
       "14      free       great\n",
       "15     asian      jewish\n",
       "16       bad        live\n",
       "17     irish         bad\n",
       "18       non         old\n",
       "19      sure       irish"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_adj = {\"Total\": top_20_adj, \"Hate Speech\": top_20_adj1}\n",
    "top_adj = pd.DataFrame(top_adj)\n",
    "top_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total</th>\n",
       "      <th>Hate Speech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>people</td>\n",
       "      <td>people</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>year</td>\n",
       "      <td>jew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>youtube</td>\n",
       "      <td>race</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>time</td>\n",
       "      <td>country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>school</td>\n",
       "      <td>time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>day</td>\n",
       "      <td>school</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>race</td>\n",
       "      <td>woman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>way</td>\n",
       "      <td>thing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>thing</td>\n",
       "      <td>way</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>country</td>\n",
       "      <td>day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>kid</td>\n",
       "      <td>negro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>video</td>\n",
       "      <td>world</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>jew</td>\n",
       "      <td>kid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>n</td>\n",
       "      <td>ape</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>woman</td>\n",
       "      <td>year</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>anyone</td>\n",
       "      <td>child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>show</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>look</td>\n",
       "      <td>youtube</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>home</td>\n",
       "      <td>place</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>post</td>\n",
       "      <td>home</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Total Hate Speech\n",
       "0    people      people\n",
       "1      year         jew\n",
       "2   youtube        race\n",
       "3      time     country\n",
       "4    school        time\n",
       "5       day      school\n",
       "6      race       woman\n",
       "7       way       thing\n",
       "8     thing         way\n",
       "9   country         day\n",
       "10      kid       negro\n",
       "11    video       world\n",
       "12      jew         kid\n",
       "13        n         ape\n",
       "14    woman        year\n",
       "15   anyone       child\n",
       "16     show           n\n",
       "17     look     youtube\n",
       "18     home       place\n",
       "19     post        home"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_noun = {\"Total\": top_20_noun, \"Hate Speech\": top_20_noun1}\n",
    "top_noun = pd.DataFrame(top_noun)\n",
    "top_noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
