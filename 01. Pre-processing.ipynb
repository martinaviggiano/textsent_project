{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. DATA IMPORT\n",
    "Hate speech dataset from a white supremacist forum:  https://github.com/Vicomtech/hate-speech-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import ntpath\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import nltk.sentiment.vader as vd\n",
    "import string\n",
    "import re\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import download\n",
    "from langdetect import detect\n",
    "import spacy\n",
    "from wordcloud import WordCloud\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, Ridge\n",
    "from sklearn.feature_selection import SelectKBest, chi2, RFE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    plot_confusion_matrix,\n",
    "    plot_precision_recall_curve,\n",
    "    plot_roc_curve,\n",
    ")\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "import sklearn as sk\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as VS\n",
    "import imblearn\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from collections import Counter\n",
    "import preprocessor as pproc\n",
    "from cleantext import clean\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marti\\anaconda3\\lib\\site-packages\\tqdm\\std.py:697: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\marti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files_paths = glob.glob(\"hate-speech-dataset/all_files/*.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files_paths = [f for f in all_files_paths if os.path.isfile(f)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files_names = [str(ntpath.basename(f)).replace(\".txt\", \"\") for f in all_files_paths]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Error removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some sentences cannot be correctly read in Windows due to error in encoding. Since they are few, we will just remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "065bd87d27ab47c6aa37d02790a87cc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=10944.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "txt_content = {}\n",
    "errors = []\n",
    "for name, path in tqdm(list(zip(all_files_names, all_files_paths))):\n",
    "    with open(path, \"r\") as txt:\n",
    "        try:\n",
    "            txt_content[name] = txt.readline().replace(\",\", \"\")\n",
    "        except Exception as ex:\n",
    "            errors.append((name, str(ex)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_list = [err[0] for err in errors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(txt_content, orient='index').reset_index()\n",
    "df.columns = [\"file_id\",\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann = pd.read_csv('hate-speech-dataset/annotations_metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann = ann[~ann['file_id'].isin(errors_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(left=ann, right=df, left_on='file_id', right_on='file_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['noHate', 'hate', 'idk/skip', 'relation'], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.loc[(data[\"label\"] != \"relation\") & (data[\"label\"] != \"idk/skip\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns=[\"file_id\", \"user_id\", \"subforum_id\", \"num_contexts\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['label'] = data.apply(lambda x: 0 if x['label'] == \"noHate\" else 1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>As of March 13th  2014  the booklet had been d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>In order to help increase the booklets downloa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>( Simply copy and paste the following text int...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Click below for a FREE download of a colorfull...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Click on the `` DOWNLOAD ( 7.42 MB ) '' green ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      0  As of March 13th  2014  the booklet had been d...\n",
       "1      0  In order to help increase the booklets downloa...\n",
       "2      0  ( Simply copy and paste the following text int...\n",
       "3      1  Click below for a FREE download of a colorfull...\n",
       "4      0  Click on the `` DOWNLOAD ( 7.42 MB ) '' green ..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. CLEANING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Removing links, tags, numbers and bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(text):\n",
    "    cList = {\n",
    "        \"n't\": \" not\",\n",
    "        \"/TD\": \" \",\n",
    "        \" PM \": \" personal message \",\n",
    "        \" pm \": \" personal message \",\n",
    "        \"PM \": \"personal message \",\n",
    "        \" Donot \": \" do not \",\n",
    "        \" MB \": \" megabytes \",\n",
    "        \"I'm\" : \"I am\"\n",
    "    }\n",
    "    \n",
    "    c_re = re.compile(\"(%s)\" % \"|\".join(cList.keys()))\n",
    "\n",
    "    return c_re.sub(lambda match: cList[match.group(0)], text)\n",
    "\n",
    "def full_text_clean(text):\n",
    "    aa = expand_contractions(text)\n",
    "    \n",
    "    bb = pproc.clean(\n",
    "        clean(pproc.clean(aa),\n",
    "              fix_unicode=True,               # fix various unicode errors\n",
    "              to_ascii=True,                  # transliterate to closest ASCII representation\n",
    "              lower=True,                     # lowercase text\n",
    "              no_line_breaks=True,           # fully strip line breaks as opposed to only normalizing them\n",
    "              no_urls=True,                  # replace all URLs with a special token\n",
    "              no_emails=True,                # replace all email addresses with a special token\n",
    "              no_phone_numbers=False,         # replace all phone numbers with a special token\n",
    "              no_numbers=False,               # replace all numbers with a special token\n",
    "              no_digits=False,                # replace all digits with a special token\n",
    "              no_currency_symbols=False,      # replace all currency symbols with a special token\n",
    "              no_punct=True,                 # remove punctuations\n",
    "              replace_with_url=\" \",\n",
    "              replace_with_email=\" \",\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    #swords = stopwords.words(\"english\")\n",
    "    #swords.extend(string.punctuation)\n",
    "    swords = string.punctuation\n",
    "\n",
    "    cc = (\n",
    "        bb.lower()\n",
    "        .replace(r\"(@[a-z0-9]+)\\w+\", \" \")\n",
    "        .replace(r\"http\\S+\", \" \")\n",
    "        .replace(r\"www\\S+\", \" \")\n",
    "        .replace(r\"com/watch\", \" \")\n",
    "        .replace(r\"\\S*[.,:;!?-]\\S*[^\\s\\.,:;!?-]\", \" \")\n",
    "        .replace(r\" th \", \" \")\n",
    "        .replace(r\"\\w*\\d\\w*\", \" \")\n",
    "        .replace(r\"rlm\", \" \")\n",
    "        .replace(r\"pttm\", \" \")\n",
    "        .replace(r\"ghlight\", \" \")\n",
    "        .replace(r\"[0-9]+(?:st| st|nd| nd|rd| rd|th| th)\", \" \")\n",
    "        .replace(r\"([^a-z \\t])\", \" \")\n",
    "        .replace(r\" +\", \" \")\n",
    "        )\n",
    "    \n",
    "    cc = \" \".join([i for i in cc.split() if not i in swords and len(i) >1 ]) # \n",
    "    \n",
    "    return cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd15621e21844a8f8f0a7cf8311dcd20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=10694.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data['text_clean'] = data[\"text\"].progress_apply(lambda x: full_text_clean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>As of March 13th  2014  the booklet had been d...</td>\n",
       "      <td>as of march the booklet had been downloaded ov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>In order to help increase the booklets downloa...</td>\n",
       "      <td>in order to help increase the booklets downloa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>( Simply copy and paste the following text int...</td>\n",
       "      <td>simply copy and paste the following text into ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Click below for a FREE download of a colorfull...</td>\n",
       "      <td>click below for free download of colorfully il...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Click on the `` DOWNLOAD ( 7.42 MB ) '' green ...</td>\n",
       "      <td>click on the download megabytes green banner link</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text  \\\n",
       "0      0  As of March 13th  2014  the booklet had been d...   \n",
       "1      0  In order to help increase the booklets downloa...   \n",
       "2      0  ( Simply copy and paste the following text int...   \n",
       "3      1  Click below for a FREE download of a colorfull...   \n",
       "4      0  Click on the `` DOWNLOAD ( 7.42 MB ) '' green ...   \n",
       "\n",
       "                                          text_clean  \n",
       "0  as of march the booklet had been downloaded ov...  \n",
       "1  in order to help increase the booklets downloa...  \n",
       "2  simply copy and paste the following text into ...  \n",
       "3  click below for free download of colorfully il...  \n",
       "4  click on the download megabytes green banner link  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['word_count_before'] = data['text'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['word_count'] = data['text_clean'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['word_cleaning'] = data['word_count_before'] - data['word_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_count_before</th>\n",
       "      <th>word_count</th>\n",
       "      <th>word_cleaning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10694.000000</td>\n",
       "      <td>10694.000000</td>\n",
       "      <td>10694.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>17.636993</td>\n",
       "      <td>14.976155</td>\n",
       "      <td>2.660838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>13.349767</td>\n",
       "      <td>11.502731</td>\n",
       "      <td>3.513010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>15.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>24.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>343.000000</td>\n",
       "      <td>286.000000</td>\n",
       "      <td>108.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       word_count_before    word_count  word_cleaning\n",
       "count       10694.000000  10694.000000   10694.000000\n",
       "mean           17.636993     14.976155       2.660838\n",
       "std            13.349767     11.502731       3.513010\n",
       "min             1.000000      0.000000      -1.000000\n",
       "25%             9.000000      7.000000       1.000000\n",
       "50%            15.000000     13.000000       2.000000\n",
       "75%            24.000000     21.000000       3.000000\n",
       "max           343.000000    286.000000     108.000000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[['word_count_before','word_count','word_cleaning']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.loc[data['word_count'] > 0, ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Lemmatizer and Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = nlp.pipe(data['text_clean'], n_process=2, batch_size=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b2aa8511deb4504922bedba1838f2ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "docs = [x for x in tqdm(pipe)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['spacy_doc'] = docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3465a8fb23074689b89b7e3c6ea90e9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=10573.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data['POS_spacy'] = data['spacy_doc'].progress_apply(lambda x: [(y.text, y.pos_) for y in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6e82d54dda64467b8d5509c59116b07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=10573.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data['lemmatized'] = data['spacy_doc'].progress_apply(lambda x: \" \".join([y.lemma_ for y in x if len(x)>1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "716843d7c592402c9f1f4a2271742754",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=10573.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data['tokens'] = data['spacy_doc'].progress_apply(lambda x: [y.text for y in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34e7f9b2846647bd8b12d710f989786e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=10573.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data['language'] = data['spacy_doc'].progress_apply(lambda x: set([y.lang_ for y in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 10573}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length = {}\n",
    "for line in data['language']:\n",
    "    if len(line) in length:\n",
    "        length[len(line)] += 1\n",
    "    else:\n",
    "        length[len(line)] = 1\n",
    "length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a204ae47c8c4723b0c8a919d4b22afd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=10573.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data['language'] = data['language'].progress_apply(lambda x: list(x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['en'], dtype=object)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['language'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_text_pos(x):\n",
    "    final_pos_text = []\n",
    "    for elem in x:\n",
    "        for pos in pos_list:\n",
    "            if elem[1] == pos:\n",
    "                final_pos_text.append(elem[0])\n",
    "    \n",
    "    return \" \".join(final_pos_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_list = [\"NOUN\"]\n",
    "data[\"NOUN\"] = data.apply(lambda x: filter_text_pos(x[\"POS_spacy\"]), axis=1)\n",
    "data['NOUN_count'] = data['NOUN'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_list = [\"PROPN\"]\n",
    "data[\"PROPN\"] = data.apply(lambda x: filter_text_pos(x[\"POS_spacy\"]), axis=1)\n",
    "data['PROPN_count'] = data['PROPN'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_list = [\"VERB\"]\n",
    "data[\"VERB\"] = data.apply(lambda x: filter_text_pos(x[\"POS_spacy\"]), axis=1)\n",
    "data['VERB_count'] = data['VERB'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_list = [\"ADJ\"]\n",
    "data[\"ADJ\"] = data.apply(lambda x: filter_text_pos(x[\"POS_spacy\"]), axis=1)\n",
    "data['ADJ_count'] = data['ADJ'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_list = [\"ADV\"]\n",
    "data[\"ADV\"] = data.apply(lambda x: filter_text_pos(x[\"POS_spacy\"]), axis=1)\n",
    "data['ADV_count'] = data['ADV'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_list = [\"PRON\"]\n",
    "data[\"PRON\"] = data.apply(lambda x: filter_text_pos(x[\"POS_spacy\"]), axis=1)\n",
    "data['PRON_count'] = data['PRON'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_list = [\"SCONJ\"]\n",
    "data[\"SCONJ\"] = data.apply(lambda x: filter_text_pos(x[\"POS_spacy\"]), axis=1)\n",
    "data['SCONJ_count'] = data['SCONJ'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_list = [\"INTJ\"]\n",
    "data[\"INTJ\"] = data.apply(lambda x: filter_text_pos(x[\"POS_spacy\"]), axis=1)\n",
    "data['INTJ_count'] = data['SCONJ'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>word_count_before</th>\n",
       "      <th>word_count</th>\n",
       "      <th>word_cleaning</th>\n",
       "      <th>spacy_doc</th>\n",
       "      <th>POS_spacy</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>tokens</th>\n",
       "      <th>...</th>\n",
       "      <th>ADJ</th>\n",
       "      <th>ADJ_count</th>\n",
       "      <th>ADV</th>\n",
       "      <th>ADV_count</th>\n",
       "      <th>PRON</th>\n",
       "      <th>PRON_count</th>\n",
       "      <th>SCONJ</th>\n",
       "      <th>SCONJ_count</th>\n",
       "      <th>INTJ</th>\n",
       "      <th>INTJ_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>As of March 13th  2014  the booklet had been d...</td>\n",
       "      <td>as of march the booklet had been downloaded ov...</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>(as, of, march, the, booklet, had, been, downl...</td>\n",
       "      <td>[(as, SCONJ), (of, ADP), (march, PROPN), (the,...</td>\n",
       "      <td>as of march the booklet have be download over ...</td>\n",
       "      <td>[as, of, march, the, booklet, had, been, downl...</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>as</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>In order to help increase the booklets downloa...</td>\n",
       "      <td>in order to help increase the booklets downloa...</td>\n",
       "      <td>34</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>(in, order, to, help, increase, the, booklets,...</td>\n",
       "      <td>[(in, ADP), (order, NOUN), (to, PART), (help, ...</td>\n",
       "      <td>in order to help increase the booklet download...</td>\n",
       "      <td>[in, order, to, help, increase, the, booklets,...</td>\n",
       "      <td>...</td>\n",
       "      <td>great uploaded</td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>it who</td>\n",
       "      <td>2</td>\n",
       "      <td>if</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>( Simply copy and paste the following text int...</td>\n",
       "      <td>simply copy and paste the following text into ...</td>\n",
       "      <td>15</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>(simply, copy, and, paste, the, following, tex...</td>\n",
       "      <td>[(simply, ADV), (copy, VERB), (and, CCONJ), (p...</td>\n",
       "      <td>simply copy and paste the follow text into -PR...</td>\n",
       "      <td>[simply, copy, and, paste, the, following, tex...</td>\n",
       "      <td>...</td>\n",
       "      <td>youtube</td>\n",
       "      <td>1</td>\n",
       "      <td>simply</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Click below for a FREE download of a colorfull...</td>\n",
       "      <td>click below for free download of colorfully il...</td>\n",
       "      <td>22</td>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "      <td>(click, below, for, free, download, of, colorf...</td>\n",
       "      <td>[(click, VERB), (below, ADV), (for, ADP), (fre...</td>\n",
       "      <td>click below for free download of colorfully il...</td>\n",
       "      <td>[click, below, for, free, download, of, colorf...</td>\n",
       "      <td>...</td>\n",
       "      <td>free zionistengineered intentional western</td>\n",
       "      <td>4</td>\n",
       "      <td>below colorfully</td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Click on the `` DOWNLOAD ( 7.42 MB ) '' green ...</td>\n",
       "      <td>click on the download megabytes green banner link</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>(click, on, the, download, megabytes, green, b...</td>\n",
       "      <td>[(click, VERB), (on, ADP), (the, DET), (downlo...</td>\n",
       "      <td>click on the download megabyte green banner link</td>\n",
       "      <td>[click, on, the, download, megabytes, green, b...</td>\n",
       "      <td>...</td>\n",
       "      <td>green</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text  \\\n",
       "0      0  As of March 13th  2014  the booklet had been d...   \n",
       "1      0  In order to help increase the booklets downloa...   \n",
       "2      0  ( Simply copy and paste the following text int...   \n",
       "3      1  Click below for a FREE download of a colorfull...   \n",
       "4      0  Click on the `` DOWNLOAD ( 7.42 MB ) '' green ...   \n",
       "\n",
       "                                          text_clean  word_count_before  \\\n",
       "0  as of march the booklet had been downloaded ov...                 16   \n",
       "1  in order to help increase the booklets downloa...                 34   \n",
       "2  simply copy and paste the following text into ...                 15   \n",
       "3  click below for free download of colorfully il...                 22   \n",
       "4  click on the download megabytes green banner link                 14   \n",
       "\n",
       "   word_count  word_cleaning  \\\n",
       "0          12              4   \n",
       "1          33              1   \n",
       "2          13              2   \n",
       "3          18              4   \n",
       "4           8              6   \n",
       "\n",
       "                                           spacy_doc  \\\n",
       "0  (as, of, march, the, booklet, had, been, downl...   \n",
       "1  (in, order, to, help, increase, the, booklets,...   \n",
       "2  (simply, copy, and, paste, the, following, tex...   \n",
       "3  (click, below, for, free, download, of, colorf...   \n",
       "4  (click, on, the, download, megabytes, green, b...   \n",
       "\n",
       "                                           POS_spacy  \\\n",
       "0  [(as, SCONJ), (of, ADP), (march, PROPN), (the,...   \n",
       "1  [(in, ADP), (order, NOUN), (to, PART), (help, ...   \n",
       "2  [(simply, ADV), (copy, VERB), (and, CCONJ), (p...   \n",
       "3  [(click, VERB), (below, ADV), (for, ADP), (fre...   \n",
       "4  [(click, VERB), (on, ADP), (the, DET), (downlo...   \n",
       "\n",
       "                                          lemmatized  \\\n",
       "0  as of march the booklet have be download over ...   \n",
       "1  in order to help increase the booklet download...   \n",
       "2  simply copy and paste the follow text into -PR...   \n",
       "3  click below for free download of colorfully il...   \n",
       "4   click on the download megabyte green banner link   \n",
       "\n",
       "                                              tokens  ...  \\\n",
       "0  [as, of, march, the, booklet, had, been, downl...  ...   \n",
       "1  [in, order, to, help, increase, the, booklets,...  ...   \n",
       "2  [simply, copy, and, paste, the, following, tex...  ...   \n",
       "3  [click, below, for, free, download, of, colorf...  ...   \n",
       "4  [click, on, the, download, megabytes, green, b...  ...   \n",
       "\n",
       "                                          ADJ ADJ_count               ADV  \\\n",
       "0                                                     0                     \n",
       "1                              great uploaded         2                     \n",
       "2                                     youtube         1            simply   \n",
       "3  free zionistengineered intentional western         4  below colorfully   \n",
       "4                                       green         1                     \n",
       "\n",
       "  ADV_count    PRON PRON_count  SCONJ SCONJ_count  INTJ INTJ_count  \n",
       "0         0                  0     as           1                1  \n",
       "1         0  it who          2     if           1                1  \n",
       "2         1                  0                  0                0  \n",
       "3         2                  0                  0                0  \n",
       "4         0                  0                  0                0  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns=[\"spacy_doc\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[[\"label\", \"text\", \"text_clean\", \"POS_spacy\", \"lemmatized\", \"tokens\", \"language\", \"word_count_before\",\"word_count\", \"word_cleaning\",\"NOUN\", \"NOUN_count\", \"PROPN\", \"PROPN_count\", \"VERB\", \"VERB_count\", \"ADJ\", \"ADJ_count\", \"ADV\", \"ADV_count\", \"PRON\", \"PRON_count\", \"SCONJ\", \"SCONJ_count\", \"INTJ\", \"INTJ_count\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 10573 entries, 0 to 10925\n",
      "Data columns (total 26 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   label              10573 non-null  int64 \n",
      " 1   text               10573 non-null  object\n",
      " 2   text_clean         10573 non-null  object\n",
      " 3   POS_spacy          10573 non-null  object\n",
      " 4   lemmatized         10573 non-null  object\n",
      " 5   tokens             10573 non-null  object\n",
      " 6   language           10573 non-null  object\n",
      " 7   word_count_before  10573 non-null  int64 \n",
      " 8   word_count         10573 non-null  int64 \n",
      " 9   word_cleaning      10573 non-null  int64 \n",
      " 10  NOUN               10573 non-null  object\n",
      " 11  NOUN_count         10573 non-null  int64 \n",
      " 12  PROPN              10573 non-null  object\n",
      " 13  PROPN_count        10573 non-null  int64 \n",
      " 14  VERB               10573 non-null  object\n",
      " 15  VERB_count         10573 non-null  int64 \n",
      " 16  ADJ                10573 non-null  object\n",
      " 17  ADJ_count          10573 non-null  int64 \n",
      " 18  ADV                10573 non-null  object\n",
      " 19  ADV_count          10573 non-null  int64 \n",
      " 20  PRON               10573 non-null  object\n",
      " 21  PRON_count         10573 non-null  int64 \n",
      " 22  SCONJ              10573 non-null  object\n",
      " 23  SCONJ_count        10573 non-null  int64 \n",
      " 24  INTJ               10573 non-null  object\n",
      " 25  INTJ_count         10573 non-null  int64 \n",
      "dtypes: int64(12), object(14)\n",
      "memory usage: 2.2+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"serialized/data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>POS_spacy</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>tokens</th>\n",
       "      <th>language</th>\n",
       "      <th>word_count_before</th>\n",
       "      <th>word_count</th>\n",
       "      <th>word_cleaning</th>\n",
       "      <th>...</th>\n",
       "      <th>ADJ</th>\n",
       "      <th>ADJ_count</th>\n",
       "      <th>ADV</th>\n",
       "      <th>ADV_count</th>\n",
       "      <th>PRON</th>\n",
       "      <th>PRON_count</th>\n",
       "      <th>SCONJ</th>\n",
       "      <th>SCONJ_count</th>\n",
       "      <th>INTJ</th>\n",
       "      <th>INTJ_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>She may or may not be a Jew but she 's certain...</td>\n",
       "      <td>she may or may not be jew but she certainly st...</td>\n",
       "      <td>[(she, PRON), (may, VERB), (or, CCONJ), (may, ...</td>\n",
       "      <td>-PRON- may or may not be jew but -PRON- certai...</td>\n",
       "      <td>[she, may, or, may, not, be, jew, but, she, ce...</td>\n",
       "      <td>en</td>\n",
       "      <td>38</td>\n",
       "      <td>34</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>other dirty</td>\n",
       "      <td>2</td>\n",
       "      <td>certainly</td>\n",
       "      <td>1</td>\n",
       "      <td>she she she her they what</td>\n",
       "      <td>6</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>I just came across this piece of garbage .</td>\n",
       "      <td>just came across this piece of garbage</td>\n",
       "      <td>[(just, ADV), (came, VERB), (across, ADP), (th...</td>\n",
       "      <td>just come across this piece of garbage</td>\n",
       "      <td>[just, came, across, this, piece, of, garbage]</td>\n",
       "      <td>en</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>just</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>I believe this can be higher since I 've read ...</td>\n",
       "      <td>believe this can be higher since ve read repor...</td>\n",
       "      <td>[(believe, VERB), (this, DET), (can, VERB), (b...</td>\n",
       "      <td>believe this can be high since ve read report ...</td>\n",
       "      <td>[believe, this, can, be, higher, since, ve, re...</td>\n",
       "      <td>en</td>\n",
       "      <td>26</td>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>higher ve free latest</td>\n",
       "      <td>4</td>\n",
       "      <td>especially</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>since that</td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>No wonder young people ca n't get jobs .</td>\n",
       "      <td>no wonder young people ca not get jobs</td>\n",
       "      <td>[(no, DET), (wonder, NOUN), (young, ADJ), (peo...</td>\n",
       "      <td>no wonder young people ca not get job</td>\n",
       "      <td>[no, wonder, young, people, ca, not, get, jobs]</td>\n",
       "      <td>en</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>young</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>National Geographic magazines in the 1950s and...</td>\n",
       "      <td>national geographic magazines in the and early...</td>\n",
       "      <td>[(national, PROPN), (geographic, ADJ), (magazi...</td>\n",
       "      <td>national geographic magazine in the and early ...</td>\n",
       "      <td>[national, geographic, magazines, in, the, and...</td>\n",
       "      <td>en</td>\n",
       "      <td>18</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>geographic replete african</td>\n",
       "      <td>3</td>\n",
       "      <td>early</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10917</th>\n",
       "      <td>0</td>\n",
       "      <td>This is what Pierre Trudeau gave Canadians once .</td>\n",
       "      <td>this is what pierre trudeau gave canadians once</td>\n",
       "      <td>[(this, DET), (is, AUX), (what, PRON), (pierre...</td>\n",
       "      <td>this be what pierre trudeau give canadian once</td>\n",
       "      <td>[this, is, what, pierre, trudeau, gave, canadi...</td>\n",
       "      <td>en</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>once</td>\n",
       "      <td>1</td>\n",
       "      <td>what</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10918</th>\n",
       "      <td>0</td>\n",
       "      <td>I ca n't find an actual picture of it anywhere...</td>\n",
       "      <td>ca not find an actual picture of it anywhere b...</td>\n",
       "      <td>[(ca, VERB), (not, PART), (find, VERB), (an, D...</td>\n",
       "      <td>ca not find an actual picture of -PRON- anywhe...</td>\n",
       "      <td>[ca, not, find, an, actual, picture, of, it, a...</td>\n",
       "      <td>en</td>\n",
       "      <td>19</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>actual</td>\n",
       "      <td>1</td>\n",
       "      <td>anywhere</td>\n",
       "      <td>1</td>\n",
       "      <td>it there it</td>\n",
       "      <td>3</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10919</th>\n",
       "      <td>0</td>\n",
       "      <td>Trudeau Saluteunknown ( -0.191 ) Another way t...</td>\n",
       "      <td>trudeau saluteunknown another way to say givin...</td>\n",
       "      <td>[(trudeau, PROPN), (saluteunknown, PROPN), (an...</td>\n",
       "      <td>trudeau saluteunknown another way to say give ...</td>\n",
       "      <td>[trudeau, saluteunknown, another, way, to, say...</td>\n",
       "      <td>en</td>\n",
       "      <td>27</td>\n",
       "      <td>19</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>primarily</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10922</th>\n",
       "      <td>0</td>\n",
       "      <td>Wish we at least had a Marine Le Pen to vote f...</td>\n",
       "      <td>wish we at least had marine le pen to vote for...</td>\n",
       "      <td>[(wish, VERB), (we, PRON), (at, ADP), (least, ...</td>\n",
       "      <td>wish -PRON- at least have marine le pen to vot...</td>\n",
       "      <td>[wish, we, at, least, had, marine, le, pen, to...</td>\n",
       "      <td>en</td>\n",
       "      <td>15</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>least marine</td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>we</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10923</th>\n",
       "      <td>0</td>\n",
       "      <td>Its like the choices are white genocide candid...</td>\n",
       "      <td>its like the choices are white genocide candid...</td>\n",
       "      <td>[(its, DET), (like, SCONJ), (the, DET), (choic...</td>\n",
       "      <td>-PRON- like the choice be white genocide candi...</td>\n",
       "      <td>[its, like, the, choices, are, white, genocide...</td>\n",
       "      <td>en</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>white</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>like</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2335 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                                               text  \\\n",
       "10         1  She may or may not be a Jew but she 's certain...   \n",
       "13         0         I just came across this piece of garbage .   \n",
       "18         0  I believe this can be higher since I 've read ...   \n",
       "25         0           No wonder young people ca n't get jobs .   \n",
       "28         0  National Geographic magazines in the 1950s and...   \n",
       "...      ...                                                ...   \n",
       "10917      0  This is what Pierre Trudeau gave Canadians once .   \n",
       "10918      0  I ca n't find an actual picture of it anywhere...   \n",
       "10919      0  Trudeau Saluteunknown ( -0.191 ) Another way t...   \n",
       "10922      0  Wish we at least had a Marine Le Pen to vote f...   \n",
       "10923      0  Its like the choices are white genocide candid...   \n",
       "\n",
       "                                              text_clean  \\\n",
       "10     she may or may not be jew but she certainly st...   \n",
       "13                just came across this piece of garbage   \n",
       "18     believe this can be higher since ve read repor...   \n",
       "25                no wonder young people ca not get jobs   \n",
       "28     national geographic magazines in the and early...   \n",
       "...                                                  ...   \n",
       "10917    this is what pierre trudeau gave canadians once   \n",
       "10918  ca not find an actual picture of it anywhere b...   \n",
       "10919  trudeau saluteunknown another way to say givin...   \n",
       "10922  wish we at least had marine le pen to vote for...   \n",
       "10923  its like the choices are white genocide candid...   \n",
       "\n",
       "                                               POS_spacy  \\\n",
       "10     [(she, PRON), (may, VERB), (or, CCONJ), (may, ...   \n",
       "13     [(just, ADV), (came, VERB), (across, ADP), (th...   \n",
       "18     [(believe, VERB), (this, DET), (can, VERB), (b...   \n",
       "25     [(no, DET), (wonder, NOUN), (young, ADJ), (peo...   \n",
       "28     [(national, PROPN), (geographic, ADJ), (magazi...   \n",
       "...                                                  ...   \n",
       "10917  [(this, DET), (is, AUX), (what, PRON), (pierre...   \n",
       "10918  [(ca, VERB), (not, PART), (find, VERB), (an, D...   \n",
       "10919  [(trudeau, PROPN), (saluteunknown, PROPN), (an...   \n",
       "10922  [(wish, VERB), (we, PRON), (at, ADP), (least, ...   \n",
       "10923  [(its, DET), (like, SCONJ), (the, DET), (choic...   \n",
       "\n",
       "                                              lemmatized  \\\n",
       "10     -PRON- may or may not be jew but -PRON- certai...   \n",
       "13                just come across this piece of garbage   \n",
       "18     believe this can be high since ve read report ...   \n",
       "25                 no wonder young people ca not get job   \n",
       "28     national geographic magazine in the and early ...   \n",
       "...                                                  ...   \n",
       "10917     this be what pierre trudeau give canadian once   \n",
       "10918  ca not find an actual picture of -PRON- anywhe...   \n",
       "10919  trudeau saluteunknown another way to say give ...   \n",
       "10922  wish -PRON- at least have marine le pen to vot...   \n",
       "10923  -PRON- like the choice be white genocide candi...   \n",
       "\n",
       "                                                  tokens language  \\\n",
       "10     [she, may, or, may, not, be, jew, but, she, ce...       en   \n",
       "13        [just, came, across, this, piece, of, garbage]       en   \n",
       "18     [believe, this, can, be, higher, since, ve, re...       en   \n",
       "25       [no, wonder, young, people, ca, not, get, jobs]       en   \n",
       "28     [national, geographic, magazines, in, the, and...       en   \n",
       "...                                                  ...      ...   \n",
       "10917  [this, is, what, pierre, trudeau, gave, canadi...       en   \n",
       "10918  [ca, not, find, an, actual, picture, of, it, a...       en   \n",
       "10919  [trudeau, saluteunknown, another, way, to, say...       en   \n",
       "10922  [wish, we, at, least, had, marine, le, pen, to...       en   \n",
       "10923  [its, like, the, choices, are, white, genocide...       en   \n",
       "\n",
       "       word_count_before  word_count  word_cleaning  ...  \\\n",
       "10                    38          34              4  ...   \n",
       "13                     9           7              2  ...   \n",
       "18                    26          23              3  ...   \n",
       "25                     9           8              1  ...   \n",
       "28                    18          15              3  ...   \n",
       "...                  ...         ...            ...  ...   \n",
       "10917                  9           8              1  ...   \n",
       "10918                 19          16              3  ...   \n",
       "10919                 27          19              8  ...   \n",
       "10922                 15          13              2  ...   \n",
       "10923                 12           9              3  ...   \n",
       "\n",
       "                              ADJ  ADJ_count         ADV  ADV_count  \\\n",
       "10                    other dirty          2   certainly          1   \n",
       "13                                         0        just          1   \n",
       "18          higher ve free latest          4  especially          1   \n",
       "25                          young          1                      0   \n",
       "28     geographic replete african          3       early          1   \n",
       "...                           ...        ...         ...        ...   \n",
       "10917                                      0        once          1   \n",
       "10918                      actual          1    anywhere          1   \n",
       "10919                                      0   primarily          1   \n",
       "10922                least marine          2                      0   \n",
       "10923                       white          1                      0   \n",
       "\n",
       "                            PRON  PRON_count       SCONJ  SCONJ_count INTJ  \\\n",
       "10     she she she her they what           6                        0        \n",
       "13                                         0                        0        \n",
       "18                                         0  since that            2        \n",
       "25                                         0                        0        \n",
       "28                                         0                        0        \n",
       "...                          ...         ...         ...          ...  ...   \n",
       "10917                       what           1                        0        \n",
       "10918                it there it           3                        0        \n",
       "10919                                      0                        0        \n",
       "10922                         we           1                        0        \n",
       "10923                                      0        like            1        \n",
       "\n",
       "       INTJ_count  \n",
       "10              0  \n",
       "13              0  \n",
       "18              2  \n",
       "25              0  \n",
       "28              0  \n",
       "...           ...  \n",
       "10917           0  \n",
       "10918           0  \n",
       "10919           0  \n",
       "10922           0  \n",
       "10923           1  \n",
       "\n",
       "[2335 rows x 26 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['text_clean'].str.contains(\"ca\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
